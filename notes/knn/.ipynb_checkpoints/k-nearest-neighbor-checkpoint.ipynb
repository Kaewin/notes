{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - K-Nearest Neighbors Notes\n",
    "\n",
    "## What is K-Nearest Neighbors?\n",
    "\n",
    "- K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression tasks.\n",
    "- KNN is a distance-based classifier that assumes smaller distances between points indicate more similarity.\n",
    "- Each column in a dataset acts as a dimension, making it easy to visualize with X and Y coordinates.\n",
    "- KNN requires labels for each point in the dataset to make predictions.\n",
    "\n",
    "## The algorithm works as follows:\n",
    "\n",
    "1. Choose a point \n",
    "2. Find the K-nearest points\n",
    "    1. K is a predefined user constant such as 1, 3, 5, or 11 \n",
    "3. Predict a label for the current point:\n",
    "    1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors \n",
    "\n",
    "## Fitting the model\n",
    "\n",
    "- KNN is a classifier that works differently from others.\n",
    "- It doesn't do much during the \"fit\" step.\n",
    "- KNN just stores training data and labels.\n",
    "- No distances are calculated during the \"fit\" step.\n",
    "- All the work is done during the \"predict\" step.\n",
    "\n",
    "## Making predictions with K\n",
    "\n",
    "- KNN algorithm predicts a class for a point during the \"predict\" step.\n",
    "- It calculates distances between the point and every point in the training set.\n",
    "- K closest points (neighbors) are found and their labels are examined.\n",
    "- Each of the K-closest points gets to 'vote' about the predicted class.\n",
    "- The majority wins and the algorithm predicts the point as whichever class has the highest count among all of the k-nearest neighbors.\n",
    "\n",
    "<img src='images/knn.gif' width = \"200\">\n",
    "\n",
    "## Distance metrics\n",
    "\n",
    "- Choosing the right distance metric is crucial when using the KNN algorithm.\n",
    "- The distance metric significantly affects the algorithm's output.\n",
    "- Euclidean distance and Minkowski distance are the standard distance metrics to consider.\n",
    "\n",
    "## Evaluating model performance\n",
    "\n",
    "- How to evaluate model performance depends on whether it's being used for classification or regression tasks\n",
    "- KNN can be used for regression and binary/multicategorical classification tasks\n",
    "- Evaluating classification performance for KNN is similar to any other classification algorithm\n",
    "- You need a set of predictions and corresponding ground-truth labels to compute evaluation metrics such as Precision, Recall, Accuracy, F1-Score, etc.\n",
    "\n",
    "### K-means\n",
    "\n",
    "- K-means algorithm is unsupervised learning clustering algorithm related to KNN.\n",
    "- K represents the number of clusters in K-means, not the number of neighbors.\n",
    "- Unlike KNN, K-means is an iterative algorithm that repeats until convergence.\n",
    "- K-means groups data points together using a distance metric to create homogeneous groupings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More On Distance Metrics:\n",
    "\n",
    "<img src='images/knn_fs.png' width = \"300\">\n",
    "\n",
    "- The K-Nearest Neighbors (KNN) algorithm is a foundational Supervised Learning algorithm.\n",
    "- Distance metrics are used to determine how similar two objects are in KNN.\n",
    "- Distance helps quantify similarity between objects.\n",
    "- Each column in a dataset is treated as a separate dimension in KNN.\n",
    "- There are multiple distance metrics available to calculate the distance between data points.\n",
    "- Learning different distance metrics is important to evaluate how similar or different data points are in KNN.\n",
    "\n",
    "## Manhattan distance\n",
    "\n",
    "<img src='images/manhattan_fs.png' width=\"300\">\n",
    "\n",
    "- Manhattan distance is a distance metric that measures the distance between two points traveling along the axes of a grid.\n",
    "- It calculates the number of units moved in the X and Y dimensions, which is the same for the red, blue, and yellow lines in the image.\n",
    "- Manhattan distance can be remembered by thinking of the famous grid of streets in Manhattan.\n",
    "- It can be calculated in any n-dimensional space by taking into account the number of units moved in each dimension and summing them.\n",
    "\n",
    "Here's the formula for Manhattan distance:\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$  \n",
    "\n",
    "Let's break this formula down:  \n",
    "\n",
    "\n",
    "- The left side of the equals sign measures the distance between two points.\n",
    "- The right side of the equals sign calculates the absolute number of units moved in each dimension and adds them up.\n",
    "- The $\\sum$ means the cumulative sum of each step in the calculation.\n",
    "- To calculate distance on a grid, movements in the opposite direction must count, so the absolute difference between them is calculated.\n",
    "- Code can easily calculate the distance between two points stored as tuples using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations of two points A and B\n",
    "A = (2, 3, 5)\n",
    "B = (1, -1, 3)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hint on turning mathematical notation into code\n",
    "\n",
    "- $\\sum$ symbol in mathematical notation can be represented as a `for` loop.\n",
    "- The math on the right of the $\\sum$ symbol tells you what the body of the `for` loop should look like.\n",
    "- The numbers on the bottom and top of the $\\sum$ sign tell you the starting and stopping indexes.\n",
    "- $n$ in the Manhattan distance equation means \"length n\", the length of the entire number of dimensions.\n",
    "- Be careful interpreting the starting dimensions, as computer scientists start counting at 0 while mathematicians start at 1.\n",
    "\n",
    "## Euclidean distance\n",
    "\n",
    "<img src='images/euclidean_fs.png' width = \"200\">\n",
    "\n",
    "- The Euclidean distance is the most common distance metric.\n",
    "- The Pythagorean theorem is at the heart of this metric.\n",
    "- The green line measures the Euclidean distance between two points by moving in a straight line.\n",
    "- The length of the green line can be calculated using the Pythagorean theorem.\n",
    "- The Euclidean distance between two points in the diagram above is approximately 8.485.\n",
    "\n",
    "### Working with more than two dimensions\n",
    "\n",
    "- You can generalize the Euclidean distance equation to any number of dimensions.\n",
    "- The formula for the Euclidean distance in a 3-dimensional space is: $d^2 = a^2 + b^2 + c^2.$\n",
    "- The Euclidean distance equation is straightforward - for each dimension, subtract one point's value from the other's, square it, and add it to the running total.\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$\n",
    "\n",
    "In Python, you can easily calculate Euclidean distance as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.58257569495584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (2, 3, 5)\n",
    "B = (1, -1, 3)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minkowski distance is a generalized distance metric across a Normed Vector Space\n",
    "- A Normed Vector Space is a collection of space where each point has been run through a function\n",
    "- Every vector must have a positive length and the zero vector outputs a length of 0\n",
    "- Manhattan and Euclidean distances are special cases of Minkowski distance\n",
    "- The function in Minkowski distance is just an exponent.\n",
    "\n",
    "If you were to define a value for the exponent, you could say that:\n",
    "\n",
    "```python \n",
    "# Manhattan Distance is the sum of all side lengths to the first power\n",
    "manhattan_distance = np.power((length_side_1**1 + length_side_2**1 + ... length_side_n**1), 1/1) \n",
    "\n",
    "# Euclidean Distance is the square root of the sum of all side lengths to the second power\n",
    "euclidean_distance = np.power((length_side_1**2 + length_side_2**2 + ... length_side_n**2), 1/2)\n",
    "\n",
    "# Minkowski Distance with a value of 3 would be the cube root of the sum of all side lengths to the third power\n",
    "minkowski_distance_3 = np.power((length_side_1**3 + length_side_2**3 + ... length_side_n**3), 1/3)\n",
    "\n",
    "# Minkowski Distance with a value of 5\n",
    "minkowski_distance_5 = np.power((length_side_1**5 + length_side_2**5 + ... length_side_n**5), 1/5)\n",
    "```\n",
    "\n",
    "> **NOTE**: You'll often see Minkowski distance used as a parameter for any distance-based machine learning algorithms inside `sklearn`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatlized Minkowski distance function\n",
    "\n",
    "Formula for Minkowski distance:\n",
    "\n",
    "$$\\large d(x,y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$\n",
    "\n",
    "- Minkowski distance is a formula used to calculate distance between two points.\n",
    "- Manhattan distance is a special case of Minkowski distance where c=1.\n",
    "- Euclidean distance is a special case of Minkowski distance where c=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between [1 2] and [4 6] is 5.00\n",
      "5.0\n",
      "Distance between [1 2] and [4 6] is 7.00\n",
      "7.0\n",
      "Distance between [1 2] and [4 6] is 4.50\n",
      "4.497941445275415\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# minkowski distance function that takes 4 arguments: two arrays, the norm to calculate, and verbose (default True)\n",
    "def distance(x1, x2, c=2, verbose=True):\n",
    "    # ensure numpy arrays\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    \n",
    "    # calculate distance\n",
    "    distance = (sum(abs(x1 - x2)**c))**(1/c)\n",
    "    \n",
    "    # print verbose\n",
    "    if verbose:\n",
    "        print(f\"Distance between {x1} and {x2} is {distance:.2f}\")\n",
    "    \n",
    "    return distance\n",
    "\n",
    "test_point_1 = (1, 2)\n",
    "test_point_2 = (4, 6)\n",
    "print(distance(test_point_1, test_point_2)) # Expected Output: 5.0\n",
    "print(distance(test_point_1, test_point_2, c=1)) # Expected Output: 7.0\n",
    "print(distance(test_point_1, test_point_2, c=3)) # Expected Output: 4.497941445275415"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Best Value Of K\n",
    "\n",
    "## Finding the optimal number of neighbors\n",
    "\n",
    "- The K-Nearest Neighbors algorithm requires selecting a value for K\n",
    "- There is no one best value for K\n",
    "- Strategies can be used to select a good or near optimal value for K\n",
    "\n",
    "## K, overfitting, and underfitting\n",
    "\n",
    "<img src='images/fit_fs.png' width = \"200\">\n",
    "\n",
    "- A smaller value of K results in a tighter fit of the model in supervised learning.\n",
    "- Overfitting can occur if the model pays too much attention to every detail and creates a complex decision boundary.\n",
    "- Conversely, underfitting occurs if the model is too simplistic.\n",
    "- A visual explanation can help understand this concept.\n",
    "- It's important to find the best value for K by iterating over multiple values and comparing performance at each step.\n",
    "\n",
    "<img src='images/best_k_fs.png' width = \"200\">\n",
    "\n",
    "As you can see from the image above, k=1 and k=3 will provide different results!\n",
    "\n",
    "## Iterating over values of K\n",
    "\n",
    "- Use odd values for k in KNN to avoid ties and guesswork\n",
    "- Fit a KNN classifier for each value of K within a minimum and maximum boundary\n",
    "- Generate predictions and evaluate performance metrics for each model\n",
    "- Compare results and choose the model with the lowest overall error or highest overall score\n",
    "- Plot the error for each value of K to find the value where the error is lowest.\n",
    "\n",
    "<img src='images/plot_fs.png' width = \"200\">\n",
    "\n",
    "## KNN and the curse of dimensionality\n",
    "\n",
    "- KNN is not ideal for large datasets or models with high dimensionality.\n",
    "- The time complexity of KNN is exponential, meaning it takes a lot of operations to complete.\n",
    "- For smaller datasets, KNN can work well due to its simplicity.\n",
    "- However, for datasets with millions of rows and thousands of columns, another algorithm may be a better choice as KNN could take years to complete.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
