{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN - K-Nearest Neighbors Notes\n",
    "\n",
    "## What is K-Nearest Neighbors?\n",
    "\n",
    "- K-Nearest Neighbors (KNN) is a supervised learning algorithm used for classification and regression tasks.\n",
    "- KNN is a distance-based classifier that assumes smaller distances between points indicate more similarity.\n",
    "- Each column in a dataset acts as a dimension, making it easy to visualize with X and Y coordinates.\n",
    "- KNN requires labels for each point in the dataset to make predictions.\n",
    "\n",
    "## The algorithm works as follows:\n",
    "\n",
    "1. Choose a point \n",
    "2. Find the K-nearest points\n",
    "    1. K is a predefined user constant such as 1, 3, 5, or 11 \n",
    "3. Predict a label for the current point:\n",
    "    1. Classification - Take the most common class of the k neighbors\n",
    "    2. Regression - Take the average target metric of the k neighbors\n",
    "    3. Both classification or regression can also be modified to use weighted averages based on the distance of the neighbors \n",
    "\n",
    "## Fitting the model\n",
    "\n",
    "- KNN is a classifier that works differently from others.\n",
    "- It doesn't do much during the \"fit\" step.\n",
    "- KNN just stores training data and labels.\n",
    "- No distances are calculated during the \"fit\" step.\n",
    "- All the work is done during the \"predict\" step.\n",
    "\n",
    "## Making predictions with K\n",
    "\n",
    "- KNN algorithm predicts a class for a point during the \"predict\" step.\n",
    "- It calculates distances between the point and every point in the training set.\n",
    "- K closest points (neighbors) are found and their labels are examined.\n",
    "- Each of the K-closest points gets to 'vote' about the predicted class.\n",
    "- The majority wins and the algorithm predicts the point as whichever class has the highest count among all of the k-nearest neighbors.\n",
    "\n",
    "<img src='images/knn.gif' width = \"200\">\n",
    "\n",
    "## Distance metrics\n",
    "\n",
    "- Choosing the right distance metric is crucial when using the KNN algorithm.\n",
    "- The distance metric significantly affects the algorithm's output.\n",
    "- Euclidean distance and Minkowski distance are the standard distance metrics to consider.\n",
    "\n",
    "## Evaluating model performance\n",
    "\n",
    "- How to evaluate model performance depends on whether it's being used for classification or regression tasks\n",
    "- KNN can be used for regression and binary/multicategorical classification tasks\n",
    "- Evaluating classification performance for KNN is similar to any other classification algorithm\n",
    "- You need a set of predictions and corresponding ground-truth labels to compute evaluation metrics such as Precision, Recall, Accuracy, F1-Score, etc.\n",
    "\n",
    "### K-means\n",
    "\n",
    "- K-means algorithm is unsupervised learning clustering algorithm related to KNN.\n",
    "- K represents the number of clusters in K-means, not the number of neighbors.\n",
    "- Unlike KNN, K-means is an iterative algorithm that repeats until convergence.\n",
    "- K-means groups data points together using a distance metric to create homogeneous groupings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More On Distance Metrics:\n",
    "\n",
    "<img src='images/knn_fs.png' width = \"300\">\n",
    "\n",
    "- The K-Nearest Neighbors (KNN) algorithm is a foundational Supervised Learning algorithm.\n",
    "- Distance metrics are used to determine how similar two objects are in KNN.\n",
    "- Distance helps quantify similarity between objects.\n",
    "- Each column in a dataset is treated as a separate dimension in KNN.\n",
    "- There are multiple distance metrics available to calculate the distance between data points.\n",
    "- Learning different distance metrics is important to evaluate how similar or different data points are in KNN.\n",
    "\n",
    "## Manhattan distance\n",
    "\n",
    "<img src='images/manhattan_fs.png' width=\"300\">\n",
    "\n",
    "- Manhattan distance is a distance metric that measures the distance between two points traveling along the axes of a grid.\n",
    "- It calculates the number of units moved in the X and Y dimensions, which is the same for the red, blue, and yellow lines in the image.\n",
    "- Manhattan distance can be remembered by thinking of the famous grid of streets in Manhattan.\n",
    "- It can be calculated in any n-dimensional space by taking into account the number of units moved in each dimension and summing them.\n",
    "\n",
    "Here's the formula for Manhattan distance:\n",
    "\n",
    "$$ \\large d(x,y) = \\sum_{i=1}^{n}|x_i - y_i | $$  \n",
    "\n",
    "Let's break this formula down:  \n",
    "\n",
    "\n",
    "- The left side of the equals sign measures the distance between two points.\n",
    "- The right side of the equals sign calculates the absolute number of units moved in each dimension and adds them up.\n",
    "- The $\\sum$ means the cumulative sum of each step in the calculation.\n",
    "- To calculate distance on a grid, movements in the opposite direction must count, so the absolute difference between them is calculated.\n",
    "- Code can easily calculate the distance between two points stored as tuples using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locations of two points A and B\n",
    "A = (2, 3, 5)\n",
    "B = (1, -1, 3)\n",
    "\n",
    "manhattan_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the absolute difference and add it\n",
    "    manhattan_distance += abs(A[i] - B[i])\n",
    "\n",
    "manhattan_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A hint on turning mathematical notation into code\n",
    "\n",
    "- $\\sum$ symbol in mathematical notation can be represented as a `for` loop.\n",
    "- The math on the right of the $\\sum$ symbol tells you what the body of the `for` loop should look like.\n",
    "- The numbers on the bottom and top of the $\\sum$ sign tell you the starting and stopping indexes.\n",
    "- $n$ in the Manhattan distance equation means \"length n\", the length of the entire number of dimensions.\n",
    "- Be careful interpreting the starting dimensions, as computer scientists start counting at 0 while mathematicians start at 1.\n",
    "\n",
    "## Euclidean distance\n",
    "\n",
    "<img src='images/euclidean_fs.png' width = \"200\">\n",
    "\n",
    "- The Euclidean distance is the most common distance metric.\n",
    "- The Pythagorean theorem is at the heart of this metric.\n",
    "- The green line measures the Euclidean distance between two points by moving in a straight line.\n",
    "- The length of the green line can be calculated using the Pythagorean theorem.\n",
    "- The Euclidean distance between two points in the diagram above is approximately 8.485.\n",
    "\n",
    "### Working with more than two dimensions\n",
    "\n",
    "- You can generalize the Euclidean distance equation to any number of dimensions.\n",
    "- The formula for the Euclidean distance in a 3-dimensional space is: $d^2 = a^2 + b^2 + c^2.$\n",
    "- The Euclidean distance equation is straightforward - for each dimension, subtract one point's value from the other's, square it, and add it to the running total.\n",
    "\n",
    "$$ \\large d(x,y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $$\n",
    "\n",
    "In Python, you can easily calculate Euclidean distance as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.58257569495584"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Locations of two points A and B\n",
    "A = (2, 3, 5)\n",
    "B = (1, -1, 3)\n",
    "\n",
    "euclidean_distance = 0\n",
    "\n",
    "# Use a for loop to iterate over each element\n",
    "for i in range(3):\n",
    "    # Calculate the difference, square, and add it\n",
    "    euclidean_distance += (A[i] - B[i]) ** 2\n",
    "\n",
    "# Square root of the final result\n",
    "euclidean_distance = sqrt(euclidean_distance)\n",
    "\n",
    "euclidean_distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minkowski distance is a generalized distance metric across a Normed Vector Space\n",
    "- A Normed Vector Space is a collection of space where each point has been run through a function\n",
    "- Every vector must have a positive length and the zero vector outputs a length of 0\n",
    "- Manhattan and Euclidean distances are special cases of Minkowski distance\n",
    "- The function in Minkowski distance is just an exponent.\n",
    "\n",
    "If you were to define a value for the exponent, you could say that:\n",
    "\n",
    "```python \n",
    "# Manhattan Distance is the sum of all side lengths to the first power\n",
    "manhattan_distance = np.power((length_side_1**1 + length_side_2**1 + ... length_side_n**1), 1/1) \n",
    "\n",
    "# Euclidean Distance is the square root of the sum of all side lengths to the second power\n",
    "euclidean_distance = np.power((length_side_1**2 + length_side_2**2 + ... length_side_n**2), 1/2)\n",
    "\n",
    "# Minkowski Distance with a value of 3 would be the cube root of the sum of all side lengths to the third power\n",
    "minkowski_distance_3 = np.power((length_side_1**3 + length_side_2**3 + ... length_side_n**3), 1/3)\n",
    "\n",
    "# Minkowski Distance with a value of 5\n",
    "minkowski_distance_5 = np.power((length_side_1**5 + length_side_2**5 + ... length_side_n**5), 1/5)\n",
    "```\n",
    "\n",
    "> **NOTE**: You'll often see Minkowski distance used as a parameter for any distance-based machine learning algorithms inside `sklearn`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generatlized Minkowski distance function\n",
    "\n",
    "Formula for Minkowski distance:\n",
    "\n",
    "$$\\large d(x,y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^c\\right)^\\frac{1}{c}$$\n",
    "\n",
    "- Minkowski distance is a formula used to calculate distance between two points.\n",
    "- Manhattan distance is a special case of Minkowski distance where c=1.\n",
    "- Euclidean distance is a special case of Minkowski distance where c=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between [1 2] and [4 6] is 5.00\n",
      "5.0\n",
      "Distance between [1 2] and [4 6] is 7.00\n",
      "7.0\n",
      "Distance between [1 2] and [4 6] is 4.50\n",
      "4.497941445275415\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# minkowski distance function that takes 4 arguments: two arrays, the norm to calculate, and verbose (default True)\n",
    "def distance(x1, x2, c=2, verbose=True):\n",
    "    # ensure numpy arrays\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    \n",
    "    # calculate distance\n",
    "    distance = (sum(abs(x1 - x2)**c))**(1/c)\n",
    "    \n",
    "    # print verbose\n",
    "    if verbose:\n",
    "        print(f\"Distance between {x1} and {x2} is {distance:.2f}\")\n",
    "    \n",
    "    return distance\n",
    "\n",
    "test_point_1 = (1, 2)\n",
    "test_point_2 = (4, 6)\n",
    "print(distance(test_point_1, test_point_2)) # Expected Output: 5.0\n",
    "print(distance(test_point_1, test_point_2, c=1)) # Expected Output: 7.0\n",
    "print(distance(test_point_1, test_point_2, c=3)) # Expected Output: 4.497941445275415"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding The Best Value Of K\n",
    "\n",
    "## Finding the optimal number of neighbors\n",
    "\n",
    "- The K-Nearest Neighbors algorithm requires selecting a value for K\n",
    "- There is no one best value for K\n",
    "- Strategies can be used to select a good or near optimal value for K\n",
    "\n",
    "## K, overfitting, and underfitting\n",
    "\n",
    "<img src='images/fit_fs.png' width = \"1000\">\n",
    "\n",
    "- A smaller value of K results in a tighter fit of the model in supervised learning.\n",
    "- Overfitting can occur if the model pays too much attention to every detail and creates a complex decision boundary.\n",
    "- Conversely, underfitting occurs if the model is too simplistic.\n",
    "- A visual explanation can help understand this concept.\n",
    "- It's important to find the best value for K by iterating over multiple values and comparing performance at each step.\n",
    "\n",
    "<img src='images/best_k_fs.png' width = \"1000\">\n",
    "\n",
    "As you can see from the image above, k=1 and k=3 will provide different results!\n",
    "\n",
    "## Iterating over values of K\n",
    "\n",
    "- Use odd values for k in KNN to avoid ties and guesswork\n",
    "- Fit a KNN classifier for each value of K within a minimum and maximum boundary\n",
    "- Generate predictions and evaluate performance metrics for each model\n",
    "- Compare results and choose the model with the lowest overall error or highest overall score\n",
    "- Plot the error for each value of K to find the value where the error is lowest.\n",
    "\n",
    "<img src='images/plot_fs.png' width=\"1000\">\n",
    "\n",
    "## KNN and the curse of dimensionality\n",
    "\n",
    "- KNN is not ideal for large datasets or models with high dimensionality.\n",
    "- The time complexity of KNN is exponential, meaning it takes a lot of operations to complete.\n",
    "- For smaller datasets, KNN can work well due to its simplicity.\n",
    "- However, for datasets with millions of rows and thousands of columns, another algorithm may be a better choice as KNN could take years to complete.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN From Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple, use a helper function, `euclidean()`, from the `spatial.distance` module of the `scipy` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `KNN` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the KNN class with two empty methods - fit and predict\n",
    "class KNN:\n",
    "    def fit(self, X_train, y_train):\n",
    "        return\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return \n",
    "\n",
    "    def closest(self, row):\n",
    "        return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the `fit()` method\n",
    "\n",
    "- When fitting a KNN classifier, you're just storing points and their labels\n",
    "- There's no actual fitting involved, just data storage\n",
    "- The stored data is used to calculate nearest neighbors when predicting\n",
    "\n",
    "The inputs for this function are:\n",
    "\n",
    "* `self`: since this will be an instance method inside the `KNN` class \n",
    "* `X_train`: an array, each row represents a _vector_ for a given point in space  \n",
    "* `y_train`: the corresponding labels for each vector in `X_train`. The label at `y_train[0]` is the label that corresponds to the vector at `X_train[0]`, and so on  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X_train, y_train):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "    \n",
    "# This line updates the knn.fit method to point to the function you've just written\n",
    "KNN.fit = fit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Three helper functions. \n",
    "\n",
    "#### `_get_distances()` function.\n",
    "\n",
    "* Take in two arguments: `self` and `x`\n",
    "* Create an empty array, `distances`, to hold all the distances you're going to calculate\n",
    "* Enumerate through every item in `self.X_train`. For each item: \n",
    "    * Use the `euclidean()` function to get the distance between x and the current point from `X_train` \n",
    "    * Create a tuple containing the index and the distance (in that order!) and append it to the `distances` array \n",
    "* Return the `distances` array when a distance has been generated for all items in `self.X_train` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_distances(self, x):\n",
    "    distances = []\n",
    "    for ind, val in enumerate(self.X_train):\n",
    "        dist_to_i = euclidean(x, val)\n",
    "        distances.append((ind, dist_to_i))\n",
    "    return distances\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class \n",
    "KNN._get_distances = _get_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `_get_k_nearest()` function \n",
    "\n",
    "* Take three arguments:\n",
    "    * `self`\n",
    "    * `dists`: an array of tuples containing (index, distance), which will be output from the `_get_distances()` method. \n",
    "    * `k`: the number of nearest neighbors you want to return\n",
    "* Sort the `dists` array by distances values, which are the second element in each tuple\n",
    "* Return the first `k` tuples from the sorted array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_k_nearest(self, dists, k):\n",
    "    sorted_dists = sorted(dists, key=lambda x: x[1])\n",
    "    return sorted_dists[:k]\n",
    "\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class \n",
    "KNN._get_k_nearest = _get_k_nearest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `_get_label_prediction()` function \n",
    "\n",
    "* Create a list containing the labels from `self.y_train` for each index in `k_nearest` (remember, each item in `k_nearest` is a tuple, and the index is stored as the first item in each tuple)\n",
    "* Get the total counts for each label (use `np.bincount()` and pass in the label array created in the previous step)\n",
    "* Get the index of the label with the highest overall count in counts (use `np.argmax()` for this, and pass in the counts created in the previous step) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_label_prediction(self, k_nearest):\n",
    "        \n",
    "    labels = [self.y_train[i] for i, _ in k_nearest]\n",
    "    counts = np.bincount(labels)\n",
    "    return np.argmax(counts)\n",
    "\n",
    "# This line attaches the function you just created as a method to KNN class\n",
    "KNN._get_label_prediction = _get_label_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can now complete the predict method. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the `predict()` method\n",
    "\n",
    "This method does all the heavy lifting for KNN, so this will be a bit more complex than the `fit()` method.\n",
    "\n",
    "* In addition to `self`, our `predict` function should take in two arguments: \n",
    "    * `X_test`: the points we want to classify\n",
    "    * `k`: which specifies the number of neighbors we should use to make the classification.  Set `k=3` as a default, but allow the user to update it if they choose \n",
    "* For each item:\n",
    "    * Calculate the distance to all points in `X_train` by using the `._get_distances()` helper method \n",
    "    * Find the k-nearest points in `X_train` by using the `._get_k_nearest()` method \n",
    "    * Use the index values contained within the tuples returned by `._get_k_nearest()` method to get the corresponding labels for each of the nearest points  \n",
    "    * Determine which class is most represented in these labels and treat that as the prediction for this point. Append the prediction to `preds` \n",
    "* Once a prediction has been generated for every item in `X_test`, return `preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X_test, k=3):\n",
    "    preds = []\n",
    "    # Iterate through each item in X_test\n",
    "    for i in X_test:\n",
    "        # Get distances between i and each item in X_train\n",
    "        dists = self._get_distances(i)\n",
    "        k_nearest = self._get_k_nearest(dists, k)\n",
    "        predicted_label = self._get_label_prediction(k_nearest)\n",
    "        preds.append(predicted_label)\n",
    "    return preds\n",
    "\n",
    "# This line updates the knn.predict method to point to the function you've just written\n",
    "KNN.predict = predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the KNN classifier\n",
    "\n",
    "Note that there are **_3 classes_** in the Iris dataset, making this a multi-categorical classification problem. This means that you can't use evaluation metrics that are meant for binary classification problems. For this, just stick to accuracy for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit KNN\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: {}\".format(accuracy_score(y_test, preds)))\n",
    "# Expected Output: Testing Accuracy: 0.9736842105263158"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN With Scikit Learn\n",
    "\n",
    "## Why use scikit-learn?\n",
    "\n",
    "- Implementing the KNN algorithm is a valuable experience but professional toolsets like scikit-learn are recommended.\n",
    "- Scikit-learn has backend optimizations that make the algorithm faster and more efficient.\n",
    "- Professional toolsets will have best-in-class implementations that a single developer or data scientist cannot rival.\n",
    "- Scikit-learn's KNN implementation is more robust and fast due to clever optimizations like caching distances.\n",
    "\n",
    "## Read the sklearn docs\n",
    "\n",
    "- Familiarize yourself with documentation for libraries and frameworks you use.\n",
    "- scikit-learn provides high-quality documentation for algorithms.\n",
    "- General documentation pages provide inputs, parameters, outputs, and caveats of any algorithm.\n",
    "- User Guides explain how the algorithm works and how to best use it, complete with sample code.\n",
    "- The scikit-learn user guide for K-Nearest Neighbors includes an image and explanation of how different parameters affect model performance.\n",
    "\n",
    "<img src='images/knn_docs.png'>\n",
    "\n",
    "[Documentation Page](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) \n",
    "\n",
    "[User Guide](https://scikit-learn.org/stable/modules/neighbors.html#classification)\n",
    "\n",
    "## Best practices\n",
    "\n",
    "- Scikit-learn has built-in functions for evaluating models with precision, accuracy, or recall scores.\n",
    "- Focus on practical questions when completing the lab, such as decisions regarding data and predictors.\n",
    "- Determine the optimal parameter values for your model and choose appropriate metrics for evaluation.\n",
    "- Assess whether there is room for improvement with your model and if potential gains are worth the time needed to achieve them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions For Improving KNN Performance - find best k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function using f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))\n",
    "    return best_k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function using log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify find_best_k to find the best value for k using log loss instead of f1-score\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=50):\n",
    "    best_k = 0\n",
    "    best_score = 0.125\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict_proba(X_test)\n",
    "        log_loss = -cross_val_score(knn, X_train, y_train, scoring=\"neg_log_loss\").mean()\n",
    "        if log_loss < best_score:\n",
    "            best_k = k\n",
    "            best_score = log_loss\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"Log Loss: {}\".format(best_score))\n",
    "    return best_k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
