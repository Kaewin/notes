{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectorization - Lab\n",
    "\n",
    "[Solutions](https://github.com/learn-co-curriculum/dsc-word-vectorization-lab)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll learn how to tokenize and vectorize text documents, create a bag of words, and apply TF-IDF vectorization. The objectives of this lab include:\n",
    "\n",
    "- Implementing tokenization and count vectorization from scratch\n",
    "- Implementing TF-IDF from scratch\n",
    "- Applying dimensionality reduction techniques to visualize vectorized text data\n",
    "\n",
    "By the end of this lab, you will have gained hands-on experience in these techniques and be able to interpret visualizations of the vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Overview\n",
    "\n",
    "In this lab, we have a corpus consisting of 20 documents containing song lyrics from Garth Brooks and Kendrick Lamar albums. The song files are stored in the `data` subdirectory of this lab's folder. Each song is stored in a separate file, named from `song1.txt` to `song20.txt`. \n",
    "\n",
    "To conveniently read in all the documents, we can use a list comprehension to create a list that contains the names of all the song files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song1.txt',\n",
       " 'song2.txt',\n",
       " 'song3.txt',\n",
       " 'song4.txt',\n",
       " 'song5.txt',\n",
       " 'song6.txt',\n",
       " 'song7.txt',\n",
       " 'song8.txt',\n",
       " 'song9.txt',\n",
       " 'song10.txt',\n",
       " 'song11.txt',\n",
       " 'song12.txt',\n",
       " 'song13.txt',\n",
       " 'song14.txt',\n",
       " 'song15.txt',\n",
       " 'song16.txt',\n",
       " 'song17.txt',\n",
       " 'song18.txt',\n",
       " 'song19.txt',\n",
       " 'song20.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [f'song{str(i)}.txt' for i in range(1,21)]\n",
    "filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import a single song to see what our text looks like so that we can make sure we clean and tokenize it correctly. \n",
    "\n",
    "Use the code in the cell below to read in the lyrics from `song18.txt` as a list of lines, just using vanilla Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Kendrick Lamar:]\\n',\n",
       " \"Two wrongs don't make us right away\\n\",\n",
       " \"Tell me something's wrong\\n\",\n",
       " 'Party all of our lives away\\n',\n",
       " 'To take you on\\n',\n",
       " '[Zacari:]\\n',\n",
       " 'Oh, baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night, all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night, all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " '[Babes Wodumo:]\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Oh my word oh my gosh oh my word (Oh my gosh)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " 'Everybody say kikiritikiki (kikiritikiki)\\n',\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " \"Ung'bambe, ung'dedele. Ung'bhasobhe, ung'gudluke\\n\",\n",
       " '[Zacari:]\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " '[Kendrick Lamar:]\\n',\n",
       " '(We go)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " 'High up (High up)\\n',\n",
       " '[?]\\n',\n",
       " '[Zacari:]\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I want you\\n',\n",
       " 'Baby I need you\\n',\n",
       " 'I wanna see you\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'Baby I wanna go out yeah\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n',\n",
       " 'All night (all night all night)\\n',\n",
       " 'All night\\n',\n",
       " \"Your body's on fire\\n\",\n",
       " 'And your drinks on ice\\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import and print song18.txt\n",
    "with open('data/song18.txt') as f:\n",
    "    test_song = f.readlines()\n",
    "    \n",
    "test_song"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data\n",
    "\n",
    "Before creating a bag of words or vectorizing each document, it is necessary to clean up the data and split each song into individual words. Tokenization is the process of breaking down text into smaller units, such as words or tokens.\n",
    "\n",
    "Consider the example sentences:\n",
    "\n",
    "`\"Two wrongs don't make us right away\\n\", \"Tell me something's wrong\\n\"`\n",
    "\n",
    "After tokenization, they would look like:\n",
    "\n",
    "`['two', 'wrongs', 'dont', 'make', 'us', 'right', 'away', 'tell', 'me', 'somethings', 'wrong']`\n",
    "\n",
    "Tokenization can be a tedious task if done manually, involving regular expressions and handling various symbols and punctuation. However, to streamline the process and focus on more important tasks, we can use existing tools. In this lab, we will leverage the `nltk` library, short for _Natural Language Tool Kit_, which provides powerful tokenization capabilities.\n",
    "\n",
    "**_NOTE:_** If you encounter an error related to missing packages when using `nltk`, it may require additional dependencies to be installed. Follow the instructions provided in the error message to install the required packages, and then rerun the cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing for Tokenization\n",
    "\n",
    "Before tokenizing the data, an additional step is required to ensure proper handling of the text. Computers have specific requirements when working with strings, and without addressing them, we may encounter issues:\n",
    "\n",
    "- Counting non-word elements: Some text may contain notes or annotations that are not part of the actual lyrics, such as `\"[Kendrick Lamar:]\"`. These elements should be removed to avoid incorrect word counts.\n",
    "- Punctuation and capitalization: Python interprets words with different capitalization or punctuation as unique, leading to separate counts. To ensure accurate counting, we need to remove punctuation and convert all words to lowercase.\n",
    "\n",
    "To address these concerns, we'll perform a manual cleaning step before tokenizing the songs. In the following cell, write a function that performs the following tasks:\n",
    "\n",
    "- Remove lines containing only `['artist names']`.\n",
    "- Join the list of strings into a single string representing the entire song.\n",
    "- Remove newline characters (`\\n`).\n",
    "- Remove the following punctuation marks: `\",.'?!()\"`\n",
    "- Convert all words to lowercase.\n",
    "\n",
    "Test the function using `test_song` to demonstrate that it successfully removes `'[Kendrick Lamar:]'` and other artist names, returns the song as a single string (not a list of strings), removes newlines and punctuation, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song(song):\n",
    "    pass\n",
    "\n",
    "clean_test_song = clean_song(test_song)\n",
    "print(clean_test_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two wrongs dont make us right away  tell me somethings wrong  party all of our lives away  to take you on  oh baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  all night all night all night  all night  your bodys on fire  and your drinks on ice  all night all night all night  all night  your bodys on fire  and your drinks on ice  oh my word oh my gosh oh my word oh my gosh  oh my word oh my gosh oh my word oh my gosh  oh my word oh my gosh oh my word oh my gosh  oh my word oh my gosh oh my word oh my gosh  everybody say kikiritikiki kikiritikiki  everybody say kikiritikiki kikiritikiki  everybody say kikiritikiki kikiritikiki  everybody say kikiritikiki kikiritikiki  ungbambe ungdedele ungbhasobhe unggudluke  ungbambe ungdedele ungbhasobhe unggudluke  ungbambe ungdedele ungbhasobhe unggudluke  ungbambe ungdedele ungbhasobhe unggudluke  baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  all night all night all night  all night  your bodys on fire  and your drinks on ice  all night all night all night  all night  your bodys on fire  and your drinks on ice  we go  high up high up  high up high up  high up high up  high up high up  high up high up  high up high up  high up high up  high up high up  baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  baby i want you  baby i need you  i wanna see you  baby i wanna go out yeah  baby i wanna go out yeah  all night all night all night  all night  your bodys on fire  and your drinks on ice  all night all night all night  all night  your bodys on fire  and your drinks on ice \n"
     ]
    }
   ],
   "source": [
    "def clean_song(song):\n",
    "    clean_lines = [line for line in song if \"[\" not in line and \"]\" not in line]\n",
    "    clean_song = \" \".join(clean_lines)\n",
    "    for symbol in \",.'?!()\":\n",
    "        clean_song = clean_song.replace(symbol, \"\")\n",
    "    clean_song = clean_song.replace(\"\\n\", \" \")\n",
    "    return clean_song.lower()\n",
    "    \n",
    "clean_test_song = clean_song(test_song)\n",
    "print(clean_test_song)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, we can use `nltk`'s `word_tokenize()` function on the song string to get a fully tokenized version of the song. Test this function on `clean_test_song` to ensure that the function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_song = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_song = word_tokenize(clean_test_song)\n",
    "tokenized_test_song[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Text: Count Vectorization\n",
    "\n",
    "To enable machine learning algorithms to process text, we need to convert it into a numerical representation. Vectorization is the process of representing text as vectors or matrices, where each element corresponds to a word in the vocabulary.\n",
    "\n",
    "Count vectorization converts text into a vector, where each element represents a unique word in the vocabulary. The vector's length is determined by the entire vocabulary, typically consisting of all the words in the English language or those present in our corpus. Each sentence can be represented by a vector, with the value of each element indicating the frequency of that word in the sentence.\n",
    "\n",
    "For example, the sentence \"I scream, you scream, we all scream for ice cream\" can be represented as a vector:\n",
    "\n",
    "| 'aardvark' | 'apple' | [...] | 'I' | 'you' | 'scream' | 'we' | 'all' | 'for' | 'ice' | 'cream' | [...] | 'xylophone' | 'zebra' |\n",
    "|:----------:|:-------:|:-----:|:---:|:-----:|:--------:|:----:|:-----:|:-----:|:-----:|:-------:|:-----:|:-----------:|:-------:|\n",
    "|      0     |    0    |   0   |  1  |   1   |     3    |   1  |   1   |   1   |   1   |    1    |   0   |      0      |    0    |\n",
    "\n",
    "This is known as a sparse representation, where most elements in the vector have a value of 0. The presence of a word in the sentence is indicated by a non-zero value, such as 1, while words not present in the sentence have a value of 0.\n",
    "\n",
    "Alternatively, we can represent the sentence as a dictionary of word frequency counts:\n",
    "\n",
    "```python\n",
    "BoW = {\n",
    "    'I': 1,\n",
    "    'you': 1,\n",
    "    'scream': 3,\n",
    "    'we': 1,\n",
    "    'all': 1,\n",
    "    'for': 1,\n",
    "    'ice': 1,\n",
    "    'cream': 1\n",
    "}\n",
    "```\n",
    "\n",
    "Both representations exemplify count vectorization, allowing us to capture the frequency of each word and represent sentences as vectors.\n",
    "\n",
    "It's important to note that count vectorization using the Bag of Words approach does not preserve the order of words in the sentence. Sentences containing the same words will result in identical vectors, even if the meanings are different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create a function that takes in a tokenized, cleaned song and returns a count vectorized representation of it as a Python dictionary.\n",
    "\n",
    "**_Hint:_**  Consider using a `set()` since you'll need each unique word in the tokenized song! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(tokenized_song):\n",
    "    pass\n",
    "\n",
    "test_vectorized = count_vectorize(tokenized_test_song)\n",
    "print(test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorize(tokenized_song):\n",
    "    unique_words = set(tokenized_song)\n",
    "\n",
    "    song_dict = {word:0 for word in unique_words}\n",
    "\n",
    "    for word in tokenized_song:\n",
    "        song_dict[word] += 1\n",
    "\n",
    "    return song_dict\n",
    "\n",
    "test_vectorized = count_vectorize(tokenized_test_song)\n",
    "print(test_vectorized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization: Summarizing Document Contents\n",
    "\n",
    "TF-IDF (Term Frequency, Inverse Document Frequency) is an advanced form of vectorization that assigns weights to each term in a document based on its uniqueness within the document and across the corpus. It allows us to summarize the document's contents using key words.\n",
    "\n",
    "TF-IDF considers the term frequency, which is the frequency of a term in a document, and the inverse document frequency, which measures the rarity of the term across all documents. A term that appears frequently in many documents is deemed less unique and less informative for distinguishing the document. On the other hand, a term that occurs frequently within a document but rarely in the rest of the corpus is considered more important in representing the document.\n",
    "\n",
    "The TF-IDF formula calculates the weight of each term by multiplying its term frequency with its inverse document frequency. We have already obtained the term frequency using Count Vectorization, as demonstrated earlier.\n",
    "\n",
    "TF-IDF vectorization enables us to capture the significance of terms within a document, highlighting their importance in representing the document's unique characteristics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IDF (Inverse Document Frequency) calculation requires all the documents in our corpus, not just an individual document. We will postpone testing this function for now.\n",
    "\n",
    "In the following cell, write a function that takes a list of tokenized songs as input. Each item in the list should be a clean, tokenized version of a song. The function should return a dictionary containing the IDF values for each word.\n",
    "\n",
    "The IDF formula is:\n",
    "\n",
    "$$\\text{IDF}(t) =  \\log_e\\left(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with } t \\text{ in it}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(list_of_token_songs):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
