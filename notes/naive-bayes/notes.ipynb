{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes's Theorem\n",
    "\n",
    "Bayes's theorem is a fundamental concept in probability theory and statistics that describes how to update the probability of an event based on new evidence or information. It is named after Thomas Bayes, an 18th-century mathematician and clergyman who introduced the theorem. Bayes's theorem is also known as Bayes's rule, Bayes's law, or Bayes's formula.\n",
    "\n",
    "1. Conditional Probability: Bayes's theorem deals with conditional probabilities, which are probabilities of an event A occurring given that another event B has already occurred. It is denoted as P(A|B), read as \"the probability of A given B.\"\n",
    "\n",
    "2. Prior Probability: The prior probability, P(A), represents the initial probability or belief in the occurrence of event A before any additional information is considered. It is the probability assigned to A without considering any specific evidence.\n",
    "\n",
    "3. Likelihood: The likelihood, P(B|A), represents the probability of observing event B given that event A has occurred. It quantifies how likely the evidence B would be if the event A were true.\n",
    "\n",
    "4. Evidence: Bayes's theorem allows us to update our belief in the occurrence of an event A based on new evidence or information represented by event B.\n",
    "\n",
    "5. Posterior Probability: The posterior probability, P(A|B), is the updated probability of event A given the occurrence of event B. It represents the revised belief in the light of the new evidence.\n",
    "\n",
    "6. Bayes's Theorem Formula: Bayes's theorem is mathematically expressed as:\n",
    "   P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "   where P(B) is the probability of event B occurring.\n",
    "\n",
    "7. Application: Bayes's theorem finds applications in various fields, including statistics, machine learning, medical diagnosis, spam filtering, and decision making under uncertainty. It allows us to incorporate new evidence to refine our beliefs and make more informed decisions.\n",
    "\n",
    "Bayes's theorem provides a powerful framework for updating probabilities based on new information and is widely used in statistical inference, Bayesian statistics, and probabilistic modeling to make predictions, estimate parameters, and reason under uncertainty.\n",
    "\n",
    "\n",
    "- Bayes's theorem describes how to update the probability of an event based on new evidence.\n",
    "- It involves conditional probabilities, including prior probability and likelihood.\n",
    "- The theorem allows us to calculate the posterior probability, which is the updated probability after considering new evidence.\n",
    "- Bayes's theorem formula: P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "- It has applications in statistics, machine learning, medical diagnosis, and decision making under uncertainty.\n",
    " \n",
    "Citations:\n",
    "1. Wasserman, L. (2013). All of Statistics: A Concise Course in Statistical Inference. Springer.\n",
    "2. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). CRC Press.\n",
    "3. Berger, J. O. (2013). Statistical Decision Theory and Bayesian Analysis (2nd ed.). Springer.\n",
    "4. McGrayne, S. B. (2011). The Theory That Would Not Die: How Bayes' Rule Cracked the Enigma Code, Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy. Yale University Press."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a simple yet effective probabilistic classification algorithm that is widely used for text classification and other machine learning tasks. It is based on Bayes' theorem with the \"naive\" assumption of feature independence. Despite its simplicity, Naive Bayes can perform well and is computationally efficient.\n",
    "\n",
    "Here are the key points to understand about Naive Bayes:\n",
    "\n",
    "1. Bayes' Theorem: Naive Bayes is built upon Bayes' theorem, which describes the probability of a hypothesis given the observed evidence. It calculates the conditional probability of a class label given the features of an instance.\n",
    "\n",
    "2. Feature Independence: The \"naive\" assumption in Naive Bayes is that the features are independent of each other, given the class label. Although this assumption is often violated in practice, Naive Bayes can still yield good results.\n",
    "\n",
    "3. Probability Model: Naive Bayes models the probability distribution of the features and class labels. It estimates the prior probabilities of the classes and the likelihood probabilities of the features given each class using training data.\n",
    "\n",
    "4. Training Process: During training, Naive Bayes estimates the prior probabilities by calculating the frequency of each class in the training data. It also estimates the likelihood probabilities by calculating the probabilities of observing each feature given the class.\n",
    "\n",
    "5. Classification Process: To classify a new instance, Naive Bayes computes the posterior probability of each class given the observed features using Bayes' theorem. The class with the highest posterior probability is assigned as the predicted class label.\n",
    "\n",
    "6. Laplace Smoothing: To handle unseen features in the test data, Naive Bayes often applies Laplace smoothing, also known as add-one smoothing. This technique adds a small value to the count of each feature, ensuring non-zero probabilities for all features.\n",
    "\n",
    "7. Text Classification: Naive Bayes is particularly popular for text classification tasks, such as spam detection, sentiment analysis, and document categorization. It treats words or features as independent variables and uses their occurrences or frequencies to make predictions.\n",
    "\n",
    "8. Advantages: Naive Bayes is known for its simplicity, efficiency, and scalability. It performs well even with limited training data and can handle high-dimensional feature spaces.\n",
    "\n",
    "9. Limitations: The main limitation of Naive Bayes is its assumption of feature independence, which may not hold in all scenarios. It may also struggle with correlated features or rare events.\n",
    "\n",
    "Despite its naive assumptions, Naive Bayes remains a useful and widely used algorithm, especially in text classification tasks. Its simplicity and efficiency make it a popular choice, and it serves as a baseline model for comparison with more complex algorithms.\n",
    "\n",
    "Note: It is important to understand that the information provided here is a general explanation of Naive Bayes. Different variations of Naive Bayes, such as Gaussian Naive Bayes and Multinomial Naive Bayes, exist to handle different types of data distributions.\n",
    "\n",
    "\n",
    "\n",
    "- Naive Bayes is a simple and popular probabilistic classification algorithm.\n",
    "- It is based on Bayes' theorem and assumes independence between features.\n",
    "- Naive Bayes calculates the conditional probability of a class given the observed features.\n",
    "- It is commonly used in text classification tasks like spam detection and sentiment analysis.\n",
    "- Model training involves estimating prior and likelihood probabilities from labeled training data.\n",
    "- Laplace smoothing is often applied to handle unseen features.\n",
    "- Naive Bayes is known for its simplicity, speed, and efficiency.\n",
    "- However, it may suffer from the assumption of feature independence and the \"zero-frequency problem\" with new features.\n",
    "\n",
    "Citations:\n",
    "1. Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.\n",
    "2. Manning, C. D., Raghavan, P., & Sch√ºtze, H. (2008). Introduction to Information Retrieval. Cambridge University Press.\n",
    "3. McCallum, A., & Nigam, K. (1998). A comparison of event models for naive Bayes text classification. In AAAI/ICML-98 Workshop on Learning for Text Categorization.\n",
    "4. Lewis, D. D., & Ringuette, M. (1994). A comparison of two learning algorithms for text categorization. In Third Annual Symposium on Document Analysis and Information Retrieval.\n",
    "5. Zhang, H. (2004). The optimality of naive Bayes. In Proceedings of the 17th International Florida Artificial Intelligence Research Society Conference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli\n",
    "\n",
    "- \"Bernoulli\" refers to a random variable or probability distribution associated with a binary outcome.\n",
    "- It models situations with two possible outcomes, such as success/failure or 1/0.\n",
    "- The Bernoulli distribution is characterized by a parameter \"p\" representing the probability of the success outcome.\n",
    "- It has a probability mass function (PMF) that assigns probabilities to specific outcomes.\n",
    "- The distribution assumes independence between trials or observations.\n",
    "- Applications include modeling binary events, binary classification, and logistic regression.\n",
    "- The Bernoulli distribution is a special case of the binomial distribution.\n",
    " \n",
    "Citations:\n",
    "1. Johnson, N. L., Kotz, S., & Balakrishnan, N. (1995). Continuous Univariate Distributions (Vol. 2, 2nd ed.). Wiley-Interscience.\n",
    "2. Ross, S. M. (2014). Introduction to Probability Models (11th ed.). Academic Press.\n",
    "3. Casella, G., & Berger, R. L. (2002). Statistical Inference (2nd ed.). Duxbury Press.\n",
    "4. Devore, J. L. (2011). Probability and Statistics for Engineering and the Sciences (8th ed.). Cengage Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial\n",
    "\n",
    "- Multinomial distribution deals with multiple outcomes or categories.\n",
    "- It describes the probabilities of observing different outcomes within multiple categories.\n",
    "- It is used for tasks involving multiple-class classification or counting occurrences across different categories.\n",
    "- The distribution has a probability mass function (PMF) and is characterized by its parameters.\n",
    "- It finds applications in various fields such as market research, genetics, and text classification.\n",
    "- It is related to other distributions like the binomial and degenerate distributions.\n",
    "- It is commonly used in statistical and machine learning applications with techniques like maximum likelihood estimation or Bayesian inference.\n",
    "\n",
    "Citations:\n",
    "1. Casella, G., & Berger, R. L. (2002). Statistical Inference (2nd ed.). Duxbury Press.\n",
    "2. Johnson, N. L., Kotz, S., & Balakrishnan, N. (1995). Continuous Multivariate Distributions (Vol. 1, 2nd ed.). Wiley-Interscience.\n",
    "3. Agresti, A. (2013). Categorical Data Analysis (3rd ed.). Wiley."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate\n",
    "\n",
    "- \"Multivariate\" refers to situations or data analysis involving multiple variables simultaneously.\n",
    "- It explores the relationships and interactions between two or more variables within a dataset.\n",
    "- Multivariate data consists of observations with values for multiple variables.\n",
    "- Multivariate analysis employs statistical techniques like regression, factor analysis, and clustering.\n",
    "- It deals with datasets of higher dimensionality, involving a larger number of variables.\n",
    "- Applications of multivariate analysis span various fields, including social sciences, economics, biology, and marketing.\n",
    "\n",
    "Citations:\n",
    "1. Johnson, R. A., & Wichern, D. W. (2007). Applied Multivariate Statistical Analysis (6th ed.). Pearson.\n",
    "2. Tabachnick, B. G., & Fidell, L. S. (2013). Using Multivariate Statistics (6th ed.). Pearson.\n",
    "3. Hair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate Data Analysis (8th ed.). Cengage Learning.\n",
    "4. Rencher, A. C., & Christensen, W. F. (2012). Methods of Multivariate Analysis (3rd ed.). Wiley-Interscience."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
