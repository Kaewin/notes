{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipelines\n",
    "\n",
    "- Pipelines are useful for data scientists to transform data, train machine learning models, and make predictions.\n",
    "- The data science process involves several steps, but a streamlined process can be achieved with the Pipeline class in scikit-learn.\n",
    "- Pipelines can integrate multiple steps of the machine learning workflow and allow for comparing different classification techniques.\n",
    "- Grid search can be integrated into the pipeline to tune hyperparameters in each of the machine learning models while avoiding data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to the training data\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score on test data\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really good blog post on the basic ideas of pipelines can be found [here](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html)\n",
    "\n",
    "# Integrating Grid Search in Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipe = Pipeline([('mms', MinMaxScaler()),\n",
    "                 ('tree', DecisionTreeClassifier(random_state=123))])\n",
    "\n",
    "# Create the grid parameter\n",
    "grid = [{'tree__max_depth': [None, 2, 6, 10], \n",
    "         'tree__min_samples_split': [5, 10]}]\n",
    "\n",
    "\n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch = GridSearchCV(estimator=pipe, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)\n",
    "\n",
    "# Fit using grid search\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the test score\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An article with a detailed workflow can be found [here](https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-2.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Machine learning pipelines create a nice workflow to combine data manipulations, preprocessing, and modeling\n",
    "* Machine learning pipelines can be used along with grid search to evaluate several parameter settings\n",
    "    * Grid search can considerably blow up computation time when computing for several parameters along with cross-validation\n",
    "    * Some models are very sensitive to hyperparameter changes, so they should be chosen with care, and even with big grids a good outcome isn't always guaranteed\n",
    "* Machine learning pipelines can also be pickled so that they can be used in the future without re-training\n",
    "* Model deployment can be something as simple as pickling a model, or a more complex approach like a cloud function that exposes model predictions through an HTTP API\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example in sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done. This pattern (preprocessing and fitting models) is very common. Although this process is fairly straightforward once you get the hang of it, **pipelines** make this process simpler, intuitive, and less error-prone. \n",
    "\n",
    "Instead of standardizing and fitting the model separately, you can do this in one step using `sklearn`'s `Pipeline()`. A pipeline takes in any number of preprocessing steps, each with `.fit()` and `transform()` methods (like `StandardScaler()` above), and a final step with a `.fit()` method (an estimator like `KNeighborsClassifier()`). The pipeline then sequentially applies the preprocessing steps and finally fits the model. Do this now.   \n",
    "\n",
    "## Build a pipeline (I) \n",
    "\n",
    "Build a pipeline with two steps: \n",
    "\n",
    "- First step: `StandardScaler()` \n",
    "- Second step (estimator): `KNeighborsClassifier()` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with StandardScaler and KNeighborsClassifier\n",
    "scaled_pipeline_1 = Pipeline([('scaler', StandardScaler()),\n",
    "                                ('clf', KNeighborsClassifier())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transform and fit the model using this pipeline to the training data (you should use `X_train` here) \n",
    "- Print the accuracy of the model on the test set (you should use `X_test` here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5775"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the training data to pipeline\n",
    "scaled_pipeline_1.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy on test set\n",
    "scaled_pipeline_1.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, this answer should match the one from above! \n",
    "\n",
    "Of course, you can also perform a grid search to determine which combination of hyperparameters can be used to build the best possible model. The way you define the pipeline still remains the same. What you need to do next is define the grid and then use `GridSearchCV()`. Let's do this now.\n",
    "\n",
    "## Build a pipeline (II)\n",
    "\n",
    "Again, build a pipeline with two steps: \n",
    "\n",
    "- First step: `StandardScaler()` named 'ss'.  \n",
    "- Second step (estimator): `RandomForestClassifier()` named 'RF'. Set `random_state=123` when instantiating the random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline with StandardScaler and RandomForestClassifier\n",
    "scaled_pipeline_2 = Pipeline([('scaler', StandardScaler()),\n",
    "                                ('RF', RandomForestClassifier())])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the defined `grid` to perform a grid search. We limited the hyperparameters and possible values to only a few values in order to limit the runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid\n",
    "grid = [{'RF__max_depth': [4, 5, 6], \n",
    "         'RF__min_samples_split': [2, 5, 10], \n",
    "         'RF__min_samples_leaf': [1, 3, 5]}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a grid search now. Use: \n",
    "- the pipeline you defined above (`scaled_pipeline_2`) as the estimator \n",
    "- the parameter `grid` \n",
    "- `'accuracy'` to evaluate the score \n",
    "- 5-fold cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid search\n",
    "gridsearch = GridSearchCV(estimator=scaled_pipeline_2, \n",
    "                          param_grid=grid, \n",
    "                          scoring='accuracy', \n",
    "                          cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the grid values and the grid search criteria, all that is left to do is fit the model to training data and then score the test set. Do this below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6025"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit the training data\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the accuracy on test set\n",
    "gridsearch.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring for pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing It All Together\n",
    "\n",
    "Here is the full preprocessing example without a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>number</th>\n",
       "      <th>number_odd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.597614</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.195229</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.792843</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B    C    number  number_odd\n",
       "0  1.224745 -0.816497 -0.5  0.000000    0.816497\n",
       "1  1.224745 -0.816497 -0.5  0.597614   -1.224745\n",
       "2 -0.816497  1.224745 -0.5  1.195229    0.816497\n",
       "3 -0.816497  1.224745 -0.5  0.000000    0.816497\n",
       "4 -0.816497 -0.816497  2.0 -1.792843   -1.224745"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data_without_pipeline(X):\n",
    "    \n",
    "    transformers = []\n",
    "\n",
    "    ### Encoding categorical data ###\n",
    "\n",
    "    # Make a transformer\n",
    "    ohe = OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    # Create transformed dataframe\n",
    "    category_encoded = ohe.fit_transform(X[[\"category\"]])\n",
    "    category_encoded = pd.DataFrame(\n",
    "        category_encoded,\n",
    "        columns=ohe.categories_[0],\n",
    "        index=X.index\n",
    "    )\n",
    "    transformers.append(ohe)\n",
    "\n",
    "    # Replace categorical data with encoded data\n",
    "    X.drop(\"category\", axis=1, inplace=True)\n",
    "    X = pd.concat([category_encoded, X], axis=1)\n",
    "\n",
    "    ### Feature engineering ###\n",
    "\n",
    "    def is_odd(data):\n",
    "        \"\"\"\n",
    "        Helper function that returns 1 if odd, 0 if even\n",
    "        \"\"\"\n",
    "        return data % 2\n",
    "\n",
    "    # Instantiate transformer\n",
    "    func_transformer = FunctionTransformer(is_odd)\n",
    "\n",
    "    # Create transformed column\n",
    "    number_odd = func_transformer.fit_transform(X[\"number\"])\n",
    "    transformers.append(func_transformer)\n",
    "\n",
    "    # Add engineered column\n",
    "    X[\"number_odd\"] = number_odd\n",
    "\n",
    "    ### Scaling ###\n",
    "\n",
    "    # Instantiate transformer\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Create transformed dataset\n",
    "    data_scaled = scaler.fit_transform(X)\n",
    "    transformers.append(scaler)\n",
    "\n",
    "    # Replace dataset with transformed one\n",
    "    X = pd.DataFrame(\n",
    "        data_scaled,\n",
    "        columns=X.columns,\n",
    "        index=X.index\n",
    "    )\n",
    "\n",
    "    return X, transformers\n",
    "\n",
    "# Reset value of example_X\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "# Test out our function\n",
    "result, transformers = preprocess_data_without_pipeline(example_X)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Refactored Pipeline Example\n",
    "\n",
    "Below is the complete pipeline (without the estimator), which produces the same output as the original full preprocessing example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>number</th>\n",
       "      <th>number_odd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.597614</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.195229</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.792843</td>\n",
       "      <td>-1.224745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B    C    number  number_odd\n",
       "0  1.224745 -0.816497 -0.5  0.000000    0.816497\n",
       "1  1.224745 -0.816497 -0.5  0.597614   -1.224745\n",
       "2 -0.816497  1.224745 -0.5  1.195229    0.816497\n",
       "3 -0.816497  1.224745 -0.5  0.000000    0.816497\n",
       "4 -0.816497 -0.816497  2.0 -1.792843   -1.224745"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_data_with_pipeline(X):\n",
    "    \n",
    "    ### Encoding categorical data ###\n",
    "    original_features_encoded = ColumnTransformer(transformers=[\n",
    "        (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"), [\"category\"])\n",
    "    ], remainder=\"passthrough\")\n",
    "    \n",
    "    ### Feature engineering ###\n",
    "    def is_odd(data):\n",
    "        \"\"\"\n",
    "        Helper function that returns 1 if odd, 0 if even\n",
    "        \"\"\"\n",
    "        return data % 2\n",
    "\n",
    "    feature_eng = ColumnTransformer(transformers=[\n",
    "        (\"add_number_odd\", FunctionTransformer(is_odd), [\"number\"])\n",
    "    ], remainder=\"drop\")\n",
    "  \n",
    "    ### Combine encoded and engineered features ###\n",
    "    feature_union = FeatureUnion(transformer_list=[\n",
    "        (\"encoded_features\", original_features_encoded),\n",
    "        (\"engineered_features\", feature_eng)\n",
    "    ])\n",
    "    \n",
    "    ### Pipeline (including scaling) ###\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"feature_union\", feature_union),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    transformed_data = pipe.fit_transform(X)\n",
    "    \n",
    "    ### Re-apply labels (optional step for readability) ###\n",
    "    encoder = original_features_encoded.named_transformers_[\"ohe\"]\n",
    "    category_labels = encoder.categories_[0]\n",
    "    all_cols = list(category_labels) + [\"number\", \"number_odd\"]\n",
    "    return pd.DataFrame(transformed_data, columns=all_cols, index=X.index), pipe\n",
    "    \n",
    "# Reset value of example_X\n",
    "example_X = example_data.drop(\"target\", axis=1)\n",
    "# Test out our new function\n",
    "result, pipe = preprocess_data_with_pipeline(example_X)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT: How to set up gridsearch for logreg\n",
    "\n",
    "To set up a grid search for a logistic regression (logreg) model, you can follow these steps:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "```\n",
    "\n",
    "2. Prepare your data:\n",
    "   - Split your data into features (X) and target variable (y).\n",
    "   - Perform any necessary data preprocessing steps, such as scaling or encoding categorical variables.\n",
    "\n",
    "3. Define the parameter grid:\n",
    "   - Create a dictionary where the keys are the hyperparameters you want to tune, and the values are the possible values for each hyperparameter. For logistic regression, common hyperparameters to tune are:\n",
    "     - `penalty`: Regularization type, such as 'l1' or 'l2'.\n",
    "     - `C`: Inverse of regularization strength, controlling the amount of regularization applied.\n",
    "     - `solver`: Optimization algorithm, such as 'liblinear' or 'lbfgs'.\n",
    "     - `max_iter`: Maximum number of iterations for the solver to converge.\n",
    "   - Example parameter grid:\n",
    "     ```python\n",
    "     param_grid = {\n",
    "         'penalty': ['l1', 'l2'],\n",
    "         'C': [0.1, 1, 10],\n",
    "         'solver': ['liblinear', 'lbfgs'],\n",
    "         'max_iter': [100, 500, 1000]\n",
    "     }\n",
    "     ```\n",
    "\n",
    "4. Create an instance of the logistic regression model:\n",
    "```python\n",
    "logreg = LogisticRegression()\n",
    "```\n",
    "\n",
    "5. Create a GridSearchCV object:\n",
    "```python\n",
    "grid_search = GridSearchCV(estimator=logreg,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5)\n",
    "```\n",
    "   - `estimator`: The estimator object to be optimized (logreg in this case).\n",
    "   - `param_grid`: The parameter grid to search over.\n",
    "   - `cv`: Number of cross-validation folds (5 in this example).\n",
    "\n",
    "6. Fit the GridSearchCV object on your data:\n",
    "```python\n",
    "grid_search.fit(X, y)\n",
    "```\n",
    "\n",
    "7. Access the best parameters and best score:\n",
    "```python\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "```\n",
    "\n",
    "After performing the grid search, the `best_params_` attribute will contain the combination of hyperparameters that yielded the best score during the cross-validation process.\n",
    "\n",
    "You can then use these best parameters to train your logistic regression model on the entire dataset or perform further evaluations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
