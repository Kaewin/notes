\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{decision\_trees}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\begin{itemize}
\item
  Decision trees are a type of model for predicting both continuous and
  categorical values.
\item
  They classify data by partitioning the sample space efficiently.
\item
  Decision trees are still one of the most powerful modeling tools in
  machine learning.
\item
  They are highly interpretable and simple to explain. \#\# Entropy and
  Information Gain
\item
  Decision trees can give different predictions based on the questions
  asked and their order.
\item
  Selecting the right questions in the right order is crucial.
\item
  Entropy and information gain are useful mechanisms for choosing the
  most promising questions in a decision tree.
\end{itemize}

\hypertarget{from-graphs-to-decision-trees}{%
\subsection{From graphs to decision
trees}\label{from-graphs-to-decision-trees}}

\begin{itemize}
\tightlist
\item
  Decision trees are a type of classifier that partitions the sample
  space recursively.
\item
  A decision tree is a directed acyclic graph with a root node and
  internal, leaf, and terminal nodes.
\item
  Internal nodes have outgoing edges while terminal nodes have no
  outgoing edges.
\item
  Directed Acyclic Graphs are collections of nodes and edges with
  specified traversal directions.
\item
  Acyclic graphs are graphs where no node can be visited twice along any
  path from one node to another.
\item
  DAGs are directed graphs with no cycles.
\item
  DAGs have a topological ordering, which is a sequence of nodes with
  every edge directed from earlier to later in the sequence.
\end{itemize}

\hypertarget{partitioning-the-sample-space}{%
\subsection{Partitioning the sample
space}\label{partitioning-the-sample-space}}

\begin{itemize}
\tightlist
\item
  Decision trees partition a sample space into sub-spaces based on
  attributes.
\item
  Internal nodes check for a condition and perform a decision, while
  terminal nodes represent a class.
\item
  Decision tree induction is related to rule induction.
\item
  Each path from the root to a leaf can be transformed into a rule.
\end{itemize}

\hypertarget{definition}{%
\subsection{Definition}\label{definition}}

\begin{itemize}
\tightlist
\item
  Decision trees are a type of classifier where each node represents a
  choice and each leaf node represents a classification.
\item
  Unknown instances are routed down the tree based on attribute values
  until they reach a leaf and are classified.
\item
  Feature importance is crucial to decision trees as selecting the
  correct feature affects the classification process.
\item
  Regression trees are represented similarly but predict continuous
  values instead of classifications.
\end{itemize}

\hypertarget{training-process}{%
\subsection{Training process}\label{training-process}}

\begin{itemize}
\tightlist
\item
  To train a decision tree for predicting target features:
\item
  Present a dataset with features and a target
\item
  Use feature selection and measures like information gain and Gini
  index to select predictors
\item
  Grow the tree until a stopping criteria is met
\item
  Use the trained tree to predict the class of new examples based on
  their features
\end{itemize}

\hypertarget{splitting-criteria}{%
\subsection{Splitting criteria}\label{splitting-criteria}}

\begin{itemize}
\tightlist
\item
  Decision trees are built using recursive binary splitting and a cost
  function to select the best split
\item
  Two algorithms commonly used to build decision trees are CART and ID3
\item
  CART uses the Gini Index as a metric while ID3 uses the entropy
  function and information gain as metrics.
\end{itemize}

\hypertarget{greedy-search}{%
\subsection{Greedy search}\label{greedy-search}}

\begin{itemize}
\tightlist
\item
  To classify data, we use decision trees with the best attribute at the
  root.
\item
  We repeat the process to create further splits until all data is
  classified.
\item
  The top-down, greedy search is used to find the best attribute.
\item
  The information gain criteria helps identify the best attribute for
  ID3 classification trees.
\item
  Decision trees always try to maximize information gain.
\item
  The attribute with the highest information gain will be split on
  first.
\end{itemize}

    \hypertarget{shannons-entropy}{%
\subsection{Shannon's Entropy}\label{shannons-entropy}}

\begin{itemize}
\tightlist
\item
  Entropy measures disorder or uncertainty.
\item
  It is named after Claude Shannon, the ``father of information
  theory''.
\item
  Information theory provides measures of uncertainty associated with
  random variables.
\item
  The amount of uncertainty is measured in bits.
\item
  The entropy of a variable is the ``amount of information'' contained
  in the variable.
\item
  The amount of information is proportional to the amount of
  ``surprise'' its reading causes.
\item
  Shannon's entropy quantifies the amount of information in a variable
  and provides a foundation for a theory around the notion of
  information.
\item
  Entropy is an indicator of how messy data is.
\item
  Higher entropy means less predictive power in data science.
\end{itemize}

\hypertarget{entropy-and-decision-trees}{%
\subsection{Entropy and Decision
Trees}\label{entropy-and-decision-trees}}

\begin{itemize}
\tightlist
\item
  Decision trees are used to group data into classes based on a target
  variable.
\item
  The goal is to maximize purity of the classes while creating clear
  leaf nodes.
\item
  Data cannot always be fully classified, but can be made tidier through
  splits using different feature variables.
\item
  Entropy is computed before and after each split to determine if it
  should be retained or stopped.
\end{itemize}

\hypertarget{calculating-entropy}{%
\subsubsection{Calculating Entropy}\label{calculating-entropy}}

\begin{itemize}
\tightlist
\item
  A dataset can contain both True and False values and be split into
  subsets according to their target value
\item
  The ratio of Trues to Falses in the dataset can be calculated using p
  = n/N and q = m/N
\item
  Entropy can be calculated using the equation E = -p . log\_2(p) - q .
  log\_2(q) and is a measure of the disorder or uncertainty in the
  dataset
\item
  When the split between target classes is at 0.5, the entropy value is
  at its maximum, 1; when the split is at 0 or 1, the entropy value is 0
\item
  The more one-sided the proportion of target classes, the less entropy;
  when the proportion is exactly equal, there is maximum entropy and
  perfect chaos
\item
  Decision Trees can be used to split the contents of a dataset into
  subsets, creating more organized subsets based on common attributes.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{from} \PY{n+nn}{math} \PY{k+kn}{import} \PY{n}{log} 

\PY{c+c1}{\PYZsh{} Write a function `entropy(pi)` to calculate total entropy in a given discrete probability distribution `pi`}
\PY{c+c1}{\PYZsh{} The function should take in a probability distribution `pi` as a list of class distributions. This should be a list of two integers, representing how many items are in each class. For example: `[4, 4]` indicates that there are four items in each class, `[10, 0]` indicates that there are 10 items in one class and 0 in the other. }
\PY{c+c1}{\PYZsh{} Calculate and return entropy according to the formula: \PYZdl{}\PYZdl{}Entropy(p) = \PYZhy{}\PYZbs{}sum (P\PYZus{}i . log\PYZus{}2(P\PYZus{}i))\PYZdl{}\PYZdl{}}
\PY{c+c1}{\PYZsh{} Make sure to avoid invalid operations like: \PYZdl{}\PYZdl{}log\PYZus{}2(0)\PYZdl{}\PYZdl{}}

\PY{k}{def} \PY{n+nf}{entropy}\PY{p}{(}\PY{n}{pi}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{    return the Entropy of a probability distribution:}
\PY{l+s+sd}{        entropy(p) = \PYZhy{} SUM (Pi * log(Pi) )}
\PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
    \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{pi}\PY{p}{:}
        \PY{n}{p} \PY{o}{=} \PY{n}{p} \PY{o}{/} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{pi}\PY{p}{)}
        \PY{k}{if} \PY{n}{p} \PY{o}{!=} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n}{p} \PY{o}{*} \PY{n}{log}\PY{p}{(}\PY{n}{p}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{total}

\PY{c+c1}{\PYZsh{} Test the function }

\PY{n+nb}{print}\PY{p}{(}\PY{n}{entropy}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Maximum Entropy e.g. a coin toss}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{entropy}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} No entropy, ignore the \PYZhy{}ve with zero , it\PYZsq{}s there due to log function}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{entropy}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} A random mix of classes}

\PY{c+c1}{\PYZsh{} 1.0}
\PY{c+c1}{\PYZsh{} \PYZhy{}0.0}
\PY{c+c1}{\PYZsh{} 0.6500224216483541}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
1.0
-0.0
0.6500224216483541
    \end{Verbatim}

    \hypertarget{generalization-of-entropy}{%
\subsubsection{Generalization of
Entropy}\label{generalization-of-entropy}}

\begin{itemize}
\tightlist
\item
  Entropy is a measure of uncertainty in a dataset
\item
  It characterizes the amount of information contained within the
  dataset
\item
  Equation to calculate entropy: H(S) = -sum(Pi*log2(Pi))
\item
  When H(S) = 0, the dataset is perfectly classified
\item
  We can easily calculate information gain for potential splits by
  knowing the amount of entropy in a subset.
\end{itemize}

\[\large H(S) = -\sum (P_i . log_2(P_i))\]

\hypertarget{information-gain}{%
\subsection{Information Gain}\label{information-gain}}

\begin{itemize}
\tightlist
\item
  Information gain is a criterion used by the ID3 algorithm to create
  decision trees.
\item
  It is calculated by comparing entropy of the parent and child nodes
  after a split.
\item
  A weighted average based on the number of samples in each class is
  used in the calculation.
\item
  The attribute with the highest information gain is chosen for the
  split.
\item
  The ID3 algorithm uses entropy to calculate information gain and pick
  the attribute to split on.
\end{itemize}

\[\large IG(A, S) = H(S) - \sum{}{p(t)H(t)}  \]

Where:

\begin{itemize}
\tightlist
\item
  \(H(S)\) is the entropy of set \(S\)
\item
  \(t\) is a subset of the attributes contained in \(A\) (we represent
  all subsets \(t\) as \(T\))
\item
  \(p(t)\) is the proportion of the number of elements in \(t\) to the
  number of elements in \(S\)
\item
  \(H(t)\) is the entropy of a given subset \(t\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Write a function `IG(D,a)` to calculate the information gain }
\PY{c+c1}{\PYZsh{} As input, the function should take in `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested}
\PY{c+c1}{\PYZsh{} Using the `entropy()` function from above, calculate the information gain as: \PYZdl{}\PYZdl{}gain(D,A) = Entropy(D) \PYZhy{} \PYZbs{}sum(\PYZbs{}frac\PYZob{}|D\PYZus{}i|\PYZcb{}\PYZob{}|D|\PYZcb{}.Entropy(D\PYZus{}i))\PYZdl{}\PYZdl{}}
\PY{c+c1}{\PYZsh{} where \PYZdl{}D\PYZus{}\PYZob{}i\PYZcb{}\PYZdl{} represents distribution of each class in `a`.}

\PY{k}{def} \PY{n+nf}{IG}\PY{p}{(}\PY{n}{D}\PY{p}{,} \PY{n}{a}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
\PY{l+s+sd}{    return the information gain:}
\PY{l+s+sd}{    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )}
\PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
    \PY{n}{total} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{a}\PY{p}{:}
        \PY{n}{total} \PY{o}{+}\PY{o}{=} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{o}{/} \PY{n+nb}{sum}\PY{p}{(}\PY{n}{D}\PY{p}{)} \PY{o}{*} \PY{n}{entropy}\PY{p}{(}\PY{n}{i}\PY{p}{)}
    \PY{k}{return} \PY{n}{entropy}\PY{p}{(}\PY{n}{D}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{total}


\PY{c+c1}{\PYZsh{} Test the function}
\PY{c+c1}{\PYZsh{} Set of example of the dataset \PYZhy{} distribution of classes}
\PY{n}{test\PYZus{}dist} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]} \PY{c+c1}{\PYZsh{} Yes, No}
\PY{c+c1}{\PYZsh{} Attribute, number of members (feature)}
\PY{n}{test\PYZus{}attr} \PY{o}{=} \PY{p}{[} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]} \PY{p}{]} \PY{c+c1}{\PYZsh{} class1, class2, class3 of attr1 according to YES/NO classes in test\PYZus{}dist}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{IG}\PY{p}{(}\PY{n}{test\PYZus{}dist}\PY{p}{,} \PY{n}{test\PYZus{}attr}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} 0.5408520829727552}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
0.5408520829727552
    \end{Verbatim}

    \hypertarget{decision-trees-in-scikit-learn}{%
\section{Decision Trees in
Scikit-Learn}\label{decision-trees-in-scikit-learn}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np} 
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd} 
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{tree} \PY{k+kn}{import} \PY{n}{DecisionTreeClassifier} 
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k+kn}{import} \PY{n}{OneHotEncoder}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k+kn}{import} \PY{n}{tree}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Load the dataset}
\PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tennis.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
    outlook  temp humidity  windy play
0     sunny   hot     high  False   no
1     sunny   hot     high   True   no
2  overcast   hot     high  False  yes
3     rainy  mild     high  False  yes
4     rainy  cool   normal  False  yes
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{outlook}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{temp}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{humidity}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{windy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
\PY{n}{y} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{play}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}

\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size} \PY{o}{=} \PY{l+m+mf}{0.3}\PY{p}{,} \PY{n}{random\PYZus{}state} \PY{o}{=} \PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} One\PYZhy{}hot encode the training data and show the resulting DataFrame with proper column names}
\PY{n}{ohe} \PY{o}{=} \PY{n}{OneHotEncoder}\PY{p}{(}\PY{p}{)}

\PY{n}{ohe}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
\PY{n}{X\PYZus{}train\PYZus{}ohe} \PY{o}{=} \PY{n}{ohe}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}\PY{o}{.}\PY{n}{toarray}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Creating this DataFrame is not necessary its only to show the result of the ohe}
\PY{n}{ohe\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}ohe}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{n}{ohe}\PY{o}{.}\PY{n}{get\PYZus{}feature\PYZus{}names}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}

\PY{n}{ohe\PYZus{}df}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
   outlook\_overcast  outlook\_rainy  outlook\_sunny  temp\_cool  temp\_hot  \textbackslash{}
0               0.0            0.0            1.0        1.0       0.0
1               1.0            0.0            0.0        0.0       1.0
2               0.0            0.0            1.0        0.0       1.0
3               0.0            1.0            0.0        0.0       0.0
4               0.0            1.0            0.0        1.0       0.0

   temp\_mild  humidity\_high  humidity\_normal  windy\_False  windy\_True
0        0.0            0.0              1.0          1.0         0.0
1        0.0            1.0              0.0          1.0         0.0
2        0.0            1.0              0.0          0.0         1.0
3        1.0            1.0              0.0          0.0         1.0
4        0.0            0.0              1.0          1.0         0.0
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Create the classifier, fit it on the training data and make predictions on the test set}
\PY{n}{clf} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}ohe}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
DecisionTreeClassifier(criterion='entropy')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}\PY{n}{ncols} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{dpi}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{)}
\PY{n}{tree}\PY{o}{.}\PY{n}{plot\PYZus{}tree}\PY{p}{(}\PY{n}{clf}\PY{p}{,}
               \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{n}{ohe\PYZus{}df}\PY{o}{.}\PY{n}{columns}\PY{p}{,} 
               \PY{n}{class\PYZus{}names}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{str}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{,}
               \PY{n}{filled} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_11_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X\PYZus{}test\PYZus{}ohe} \PY{o}{=} \PY{n}{ohe}\PY{o}{.}\PY{n}{transform}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{y\PYZus{}preds} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test\PYZus{}ohe}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}preds}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy:  0.6
    \end{Verbatim}

    \hypertarget{hyperparameter-tuning-and-pruning-in-decision-trees}{%
\section{Hyperparameter Tuning and Pruning in Decision
Trees}\label{hyperparameter-tuning-and-pruning-in-decision-trees}}

    \hypertarget{hyperparameter-optimization}{%
\section{Hyperparameter
Optimization}\label{hyperparameter-optimization}}

\begin{itemize}
\tightlist
\item
  Hyperparameters are parameters set before the learning process in
  machine learning.
\item
  Different model training algorithms require different hyperparameters.
\item
  Lasso is an algorithm that adds a regularization hyperparameter to
  ordinary least squares regression.
\item
  Hyperparameters affect the predictive performance and computational
  complexity of the model. Tree pruning
\item
  We can optimize a decision tree classifier by tweaking parameters
  before learning takes place.
\item
  Growing a decision tree beyond a certain level of complexity can lead
  to overfitting.
\item
  Tree pruning can adjust the amount of overfitting or underfitting to
  optimize for accuracy, precision, and/or recall.
\item
  Tree pruning can be done through adjusting maximum depth, minimum leaf
  sample size, maximum leaf nodes, and maximum features.
\end{itemize}

Let's look at a few hyperparameters and learn about their impact on
classifier performance:

\texttt{max\_depth}

\begin{itemize}
\tightlist
\item
  The max\_depth parameter is the first one to tune for decision trees.
\item
  A tree that is too deep may result in overfitting and difficulty
  generalizing on unseen data.
\item
  A tree that is too shallow may result in underfitting and low accuracy
  for both training and test samples.
\item
  The training accuracy keeps rising with greater depths, but the
  validation accuracy falls constantly.
\item
  Finding the sweet spot, such as a depth of 4, is the first
  hyperparameter to tune.
\end{itemize}

\texttt{min\_samples\_split}

\begin{itemize}
\tightlist
\item
  The min\_samples\_split hyperparameter sets the minimum number of
  samples required to split an internal node.
\item
  Increasing this parameter value makes the tree more constrained and
  considers more samples at each node.
\item
  Training and test accuracy stabilize at a certain minimum sample split
  size, even if we increase the size of the split.
\item
  Identifying the optimal sample size during training is imperative to
  avoid a complex model with similar accuracy to a much simpler model.
\item
  Large values for min\_samples\_split and max\_depth can create
  complex, dense, and long trees that are computationally expensive,
  especially for large datasets.
\end{itemize}

\texttt{min\_samples\_leaf}

\begin{itemize}
\tightlist
\item
  Min\_samples\_leaf is a hyperparameter used to set the minimum number
  of samples in a leaf node.
\item
  It guarantees a minimum number of samples in a leaf, while
  min\_samples\_split can create arbitrary small leaves.
\item
  Increasing the min\_samples\_leaf value after an optimal point reduces
  accuracy due to underfitting.
\item
  An internal node will have further splits, while a leaf is a node
  without any children.
\item
  If one of the leaves resulted will have less than the minimum number
  of samples required to be at a leaf node, the split won't be allowed.
\end{itemize}

\hypertarget{are-there-more-hyperparameters}{%
\subsection{Are there more
hyperparameters?}\label{are-there-more-hyperparameters}}

\begin{itemize}
\tightlist
\item
  Scikit-learn has other hyperparameters for fine-tuning the learning
  process.
\item
  Consult the official doc for more details.
\item
  The hyperparameters mentioned here are related to complexity in
  decision trees.
\item
  These hyperparameters are normally tuned when growing trees.
\end{itemize}

    \hypertarget{additional-resources}{%
\subsection{Additional resources:}\label{additional-resources}}

\href{https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview}{overview
of hyperparamter tuning}

\href{https://towardsdatascience.com/demystifying-hyper-parameter-tuning-acb83af0258f}{demystifying
hyperparameter tuning}

\href{https://www.displayr.com/machine-learning-pruning-decision-trees/}{pruning
decision trees}

    \hypertarget{maximum-tree-depth}{%
\subsection{Maximum tree depth}\label{maximum-tree-depth}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Identify the optimal tree depth for given data}
\PY{c+c1}{\PYZsh{} create an array for max\PYZus{}depth values ranging from 1 to 32}
\PY{n}{max\PYZus{}depths} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} in a loop train the classifier for each depth value (32 runs)}
\PY{c+c1}{\PYZsh{} calculate the training and test AUC scores for each run}
\PY{c+c1}{\PYZsh{} plot a graph to show underfitting/overfitting and the optimal value}
\PY{n}{train\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{max\PYZus{}depth} \PY{o+ow}{in} \PY{n}{max\PYZus{}depths}\PY{p}{:}
    \PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{n}{max\PYZus{}depth}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
    \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
    \PY{n}{train\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{train\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{test\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{max\PYZus{}depths}\PY{p}{,} \PY{n}{train\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{max\PYZus{}depths}\PY{p}{,} \PY{n}{test\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tree depth}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{minimum-sample-split}{%
\subsection{minimum sample split}\label{minimum-sample-split}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Identify the optimal min\PYZhy{}samples\PYZhy{}split for given data}
\PY{c+c1}{\PYZsh{} Identify the optimal min\PYZhy{}samples\PYZhy{}split for given data}
\PY{n}{min\PYZus{}samples\PYZus{}splits} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{min\PYZus{}samples\PYZus{}split} \PY{o+ow}{in} \PY{n}{min\PYZus{}samples\PYZus{}splits}\PY{p}{:}
    \PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=}\PY{n}{min\PYZus{}samples\PYZus{}split}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
    \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
    \PY{n}{train\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=}    \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{train\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{test\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{min\PYZus{}samples\PYZus{}splits}\PY{p}{,} \PY{n}{train\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{min\PYZus{}samples\PYZus{}splits}\PY{p}{,} \PY{n}{test\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min. Sample splits}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{minimum-sample-leafs}{%
\subsection{minimum sample leafs}\label{minimum-sample-leafs}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Calculate the optimal value for minimum sample leafs}
\PY{c+c1}{\PYZsh{} Calculate the optimal value for minimum sample leafs}
\PY{n}{min\PYZus{}samples\PYZus{}leafs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{endpoint}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{train\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{min\PYZus{}samples\PYZus{}leaf} \PY{o+ow}{in} \PY{n}{min\PYZus{}samples\PYZus{}leafs}\PY{p}{:}
    \PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=}\PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
    \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
    \PY{n}{train\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{train\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{test\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}    
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{min\PYZus{}samples\PYZus{}leafs}\PY{p}{,} \PY{n}{train\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{min\PYZus{}samples\PYZus{}leafs}\PY{p}{,} \PY{n}{test\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Min. Sample Leafs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{maximum-features}{%
\subsection{Maximum features}\label{maximum-features}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Find the best value for optimal maximum feature size}
\PY{n}{max\PYZus{}features} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\PY{n}{train\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{test\PYZus{}results} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{max\PYZus{}feature} \PY{o+ow}{in} \PY{n}{max\PYZus{}features}\PY{p}{:}
    \PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{n}{max\PYZus{}feature}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
    \PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
    \PY{n}{train\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{train\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{train\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}
    \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
    \PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
    \PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
    \PY{n}{test\PYZus{}results}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{roc\PYZus{}auc}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{p}{,} \PY{n}{train\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Train AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{max\PYZus{}features}\PY{p}{,} \PY{n}{test\PYZus{}results}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Test AUC}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AUC score}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{max features}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{re-train-with-chosen-values}{%
\subsection{re-train with chosen
values}\label{re-train-with-chosen-values}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Train a classifier with optimal values identified above}
\PY{n}{dt} \PY{o}{=} \PY{n}{DecisionTreeClassifier}\PY{p}{(}\PY{n}{criterion}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{entropy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                           \PY{n}{max\PYZus{}features}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,}
                           \PY{n}{max\PYZus{}depth}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,}
                           \PY{n}{min\PYZus{}samples\PYZus{}split}\PY{o}{=}\PY{l+m+mf}{0.7}\PY{p}{,}
                           \PY{n}{min\PYZus{}samples\PYZus{}leaf}\PY{o}{=}\PY{l+m+mf}{0.25}\PY{p}{,} 
                           \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{SEED}\PY{p}{)}
\PY{n}{dt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
\PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{dt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{thresholds} \PY{o}{=} \PY{n}{roc\PYZus{}curve}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}
\PY{n}{roc\PYZus{}auc} \PY{o}{=} \PY{n}{auc}\PY{p}{(}\PY{n}{false\PYZus{}positive\PYZus{}rate}\PY{p}{,} \PY{n}{true\PYZus{}positive\PYZus{}rate}\PY{p}{)}
\PY{n}{roc\PYZus{}auc}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{regression-with-cart-trees}{%
\section{Regression with CART Trees}\label{regression-with-cart-trees}}

    \hypertarget{recursive-partitioning}{%
\subsection{Recursive partitioning}\label{recursive-partitioning}}

\begin{itemize}
\tightlist
\item
  Linear regression is a global model that can be difficult and
  computationally expensive to assemble for data with complex features
  and nonlinear relations.
\item
  Nonlinear regressions can be handled by partitioning the sample space
  into smaller regions using recursive partitioning.
\item
  In regression trees, each leaf node represents a cell of the partition
  where a simple regression can be accurately fit to the data.
\item
  The data is continuously subsetted down to smaller, more specific
  subsets until the simplest regression model can be built for the most
  specific subset.
\end{itemize}

\hypertarget{simple-local-models}{%
\subsection{Simple local models}\label{simple-local-models}}

\begin{itemize}
\tightlist
\item
  Simple regression models for each partition do not calculate the
  actual regression model, but instead use the sample mean of the
  dependent variable for that partition.
\item
  This works well in practice, and has advantages including easier
  interpretation and faster inference.
\item
  Decision trees have decision boundaries that are always horizontal or
  vertical because of the boolean logic used.
\item
  The tree correctly represents the interaction between Horsepower and
  Wheelbase.
\item
  All effort should go into finding a good partitioning of the data.
\end{itemize}

\hypertarget{cart-training-algorithm}{%
\subsubsection{CART training algorithm}\label{cart-training-algorithm}}

\begin{itemize}
\tightlist
\item
  The lab focuses on the CART algorithm for regression.
\item
  The algorithm builds a binary tree where each non-leaf node has two
  children.
\item
  The training examples are split into two subsets using a feature set
  and threshold.
\item
  The algorithm selects the split that produces the smallest mean
  squared error at each node.
\item
  The cost function used for selecting parameters is based on the number
  of samples on each subset and the MSE of each subset.
\item
  The process is repeated until the maximum allowable depth or minimum
  number of samples is reached.
\item
  New examples can be classified by navigating through the tree.
\end{itemize}

So at each step, the algorithm selects the parameters \(\theta\) that
minimizes the following cost function:

\begin{equation}
J(D, \theta) = \frac{n_{left}}{n_{total}} MSE_{left} + \frac{n_{right}}{n_{total}} MSE_{right}
\end{equation}

\begin{itemize}
\tightlist
\item
  \(D\): remaining training examples\\
\item
  \(n_{total}\) : number of remaining training examples
\item
  \(\theta = (f, t_f)\): feature and feature threshold
\item
  \(n_{left}/n_{right}\): number of samples in the left/right subset
\item
  \(MSE_{left}/MSE_{right}\): MSE of the left/right subset
\end{itemize}

\hypertarget{mean-squared-error-mse}{%
\subsubsection{Mean Squared Error (MSE)}\label{mean-squared-error-mse}}

When performing regression with CART trees (i.e.~the target values are
continuous) we can evaluate a split using its MSE. The MSE of node \(m\)
is computed as follows:

\begin{equation}
\hat{y}_m = \frac{1}{n_{m}} \sum_{i \in D_m} y_i
\end{equation}

\begin{equation}
MSE_m = \frac{1}{n_{m}} \sum_{i \in D_m} (y_i - \hat{y}_m)^2
\end{equation}

\begin{itemize}
\tightlist
\item
  \(D_m\): training examples in node \(m\)
\item
  \(n_{m}\) : total number of training examples in node \(m\)
\item
  \(y_i\): target value of \(i-\)th example
\item
  CART trees are used for regression where target values are continuous.
\item
  Split evaluation is done using Mean Squared Error (MSE).
\item
  MSE of a node is computed using the formula provided.
\item
  \(D_m\) refers to training examples in node \(m\) and \(n_m\) refers
  to the total number of training examples in node \(m\).
\item
  \(y_i\) refers to the target value of the \(i\)-th example.
\end{itemize}

    \hypertarget{key-takeaways}{%
\section{Key Takeaways}\label{key-takeaways}}

The key takeaways from this section include:

\begin{itemize}
\tightlist
\item
  Decision trees can be used for both categorization and regression
  tasks
\item
  They are a powerful and interpretable technique for many machine
  learning problems (especially when combined with ensemble methods)
\item
  Decision trees are a form of Directed Acyclic Graphs (DAGs) - you
  traverse them in a specified direction, and there are no ``loops'' in
  the graphs to go backward
\item
  Algorithms for generating decision trees are designed to maximize the
  information gain from each split
\item
  A popular algorithm for generating decision trees is ID3 - the
  Iterative Dichotomiser 3 algorithm
\item
  There are several hyperparameters for decision trees to reduce
  overfitting - including maximum depth, minimum samples to split a node
  that is currently a leaf, minimum leaf sample size, maximum leaf
  nodes, and maximum features
\end{itemize}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
