{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "- Decision trees are a type of model for predicting both continuous and categorical values.\n",
    "- They classify data by partitioning the sample space efficiently.\n",
    "- Decision trees are still one of the most powerful modeling tools in machine learning.\n",
    "- They are highly interpretable and simple to explain.\n",
    "## Entropy and Information Gain\n",
    "\n",
    "- Decision trees can give different predictions based on the questions asked and their order.\n",
    "- Selecting the right questions in the right order is crucial.\n",
    "- Entropy and information gain are useful mechanisms for choosing the most promising questions in a decision tree.\n",
    "\n",
    "## From graphs to decision trees\n",
    "- Decision trees are a type of classifier that partitions the sample space recursively.\n",
    "- A decision tree is a directed acyclic graph with a root node and internal, leaf, and terminal nodes.\n",
    "- Internal nodes have outgoing edges while terminal nodes have no outgoing edges.\n",
    "- Directed Acyclic Graphs are collections of nodes and edges with specified traversal directions.\n",
    "- Acyclic graphs are graphs where no node can be visited twice along any path from one node to another.\n",
    "- DAGs are directed graphs with no cycles.\n",
    "- DAGs have a topological ordering, which is a sequence of nodes with every edge directed from earlier to later in the sequence.\n",
    "\n",
    "\n",
    "## Partitioning the sample space\n",
    "\n",
    "<img src=\"images/dt1.png\" width=650>\n",
    "\n",
    "- Decision trees partition a sample space into sub-spaces based on attributes.\n",
    "- Internal nodes check for a condition and perform a decision, while terminal nodes represent a class.\n",
    "- Decision tree induction is related to rule induction.\n",
    "- Each path from the root to a leaf can be transformed into a rule.\n",
    "\n",
    "## Definition\n",
    "\n",
    "<img src=\"images/dt2.png\" width=650>\n",
    "\n",
    "- Decision trees are a type of classifier where each node represents a choice and each leaf node represents a classification.\n",
    "- Unknown instances are routed down the tree based on attribute values until they reach a leaf and are classified.\n",
    "- Feature importance is crucial to decision trees as selecting the correct feature affects the classification process.\n",
    "- Regression trees are represented similarly but predict continuous values instead of classifications.\n",
    "\n",
    "## Training process\n",
    "\n",
    "<img src=\"images/dt3.png\" width=650>\n",
    "\n",
    "- To train a decision tree for predicting target features:\n",
    "- Present a dataset with features and a target\n",
    "- Use feature selection and measures like information gain and Gini index to select predictors\n",
    "- Grow the tree until a stopping criteria is met\n",
    "- Use the trained tree to predict the class of new examples based on their features\n",
    "\n",
    "## Splitting criteria\n",
    "\n",
    "- Decision trees are built using recursive binary splitting and a cost function to select the best split\n",
    "- Two algorithms commonly used to build decision trees are CART and ID3\n",
    "- CART uses the Gini Index as a metric while ID3 uses the entropy function and information gain as metrics.\n",
    "\n",
    "## Greedy search \n",
    "\n",
    "- To classify data, we use decision trees with the best attribute at the root.\n",
    "- We repeat the process to create further splits until all data is classified.\n",
    "- The top-down, greedy search is used to find the best attribute.\n",
    "- The information gain criteria helps identify the best attribute for ID3 classification trees.\n",
    "- Decision trees always try to maximize information gain.\n",
    "- The attribute with the highest information gain will be split on first.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's Entropy\n",
    "\n",
    "- Entropy measures disorder or uncertainty.\n",
    "- It is named after Claude Shannon, the \"father of information theory\".\n",
    "- Information theory provides measures of uncertainty associated with random variables.\n",
    "- The amount of uncertainty is measured in bits.\n",
    "- The entropy of a variable is the \"amount of information\" contained in the variable.\n",
    "- The amount of information is proportional to the amount of \"surprise\" its reading causes.\n",
    "- Shannon's entropy quantifies the amount of information in a variable and provides a foundation for a theory around the notion of information.\n",
    "- Entropy is an indicator of how messy data is.\n",
    "- Higher entropy means less predictive power in data science.\n",
    "\n",
    "## Entropy and Decision Trees\n",
    "\n",
    "<img src=\"images/split.png\" width=500>\n",
    "\n",
    "- Decision trees are used to group data into classes based on a target variable.\n",
    "- The goal is to maximize purity of the classes while creating clear leaf nodes.\n",
    "- Data cannot always be fully classified, but can be made tidier through splits using different feature variables.\n",
    "- Entropy is computed before and after each split to determine if it should be retained or stopped.\n",
    "\n",
    "###  Calculating Entropy\n",
    "\n",
    "<img src=\"images/ent.png\" width=400>\n",
    "\n",
    "- A dataset can contain both True and False values and be split into subsets according to their target value\n",
    "- The ratio of Trues to Falses in the dataset can be calculated using p = n/N and q = m/N\n",
    "- Entropy can be calculated using the equation E = -p . log_2(p) - q . log_2(q) and is a measure of the disorder or uncertainty in the dataset\n",
    "- When the split between target classes is at 0.5, the entropy value is at its maximum, 1; when the split is at 0 or 1, the entropy value is 0\n",
    "- The more one-sided the proportion of target classes, the less entropy; when the proportion is exactly equal, there is maximum entropy and perfect chaos\n",
    "- Decision Trees can be used to split the contents of a dataset into subsets, creating more organized subsets based on common attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.0\n",
      "0.6500224216483541\n"
     ]
    }
   ],
   "source": [
    "from math import log \n",
    "\n",
    "# Write a function `entropy(pi)` to calculate total entropy in a given discrete probability distribution `pi`\n",
    "# The function should take in a probability distribution `pi` as a list of class distributions. This should be a list of two integers, representing how many items are in each class. For example: `[4, 4]` indicates that there are four items in each class, `[10, 0]` indicates that there are 10 items in one class and 0 in the other. \n",
    "# Calculate and return entropy according to the formula: $$Entropy(p) = -\\sum (P_i . log_2(P_i))$$\n",
    "# Make sure to avoid invalid operations like: $$log_2(0)$$\n",
    "\n",
    "def entropy(pi):\n",
    "    '''\n",
    "    return the Entropy of a probability distribution:\n",
    "        entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    '''\n",
    "    total = 0\n",
    "    for p in pi:\n",
    "        p = p / sum(pi)\n",
    "        if p != 0:\n",
    "            total += p * log(p, 2)\n",
    "    return -total\n",
    "\n",
    "# Test the function \n",
    "\n",
    "print(entropy([1, 1])) # Maximum Entropy e.g. a coin toss\n",
    "print(entropy([0, 6])) # No entropy, ignore the -ve with zero , it's there due to log function\n",
    "print(entropy([2, 10])) # A random mix of classes\n",
    "\n",
    "# 1.0\n",
    "# -0.0\n",
    "# 0.6500224216483541"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization of Entropy \n",
    "\n",
    "- Entropy is a measure of uncertainty in a dataset\n",
    "- It characterizes the amount of information contained within the dataset\n",
    "- Equation to calculate entropy: H(S) = -sum(Pi*log2(Pi))\n",
    "- When H(S) = 0, the dataset is perfectly classified\n",
    "- We can easily calculate information gain for potential splits by knowing the amount of entropy in a subset.\n",
    "\n",
    "$$\\large H(S) = -\\sum (P_i . log_2(P_i))$$\n",
    "\n",
    "## Information Gain \n",
    "\n",
    "- Information gain is a criterion used by the ID3 algorithm to create decision trees.\n",
    "- It is calculated by comparing entropy of the parent and child nodes after a split.\n",
    "- A weighted average based on the number of samples in each class is used in the calculation.\n",
    "- The attribute with the highest information gain is chosen for the split.\n",
    "- The ID3 algorithm uses entropy to calculate information gain and pick the attribute to split on.\n",
    "\n",
    "$$\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $H(S)$ is the entropy of set $S$\n",
    "* $t$ is a subset of the attributes contained in $A$ (we represent all subsets $t$ as $T$)\n",
    "* $p(t)$ is the proportion of the number of elements in $t$ to the number of elements in $S$\n",
    "* $H(t)$ is the entropy of a given subset $t$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408520829727552\n"
     ]
    }
   ],
   "source": [
    "# Write a function `IG(D,a)` to calculate the information gain \n",
    "# As input, the function should take in `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested\n",
    "# Using the `entropy()` function from above, calculate the information gain as: $$gain(D,A) = Entropy(D) - \\sum(\\frac{|D_i|}{|D|}.Entropy(D_i))$$\n",
    "# where $D_{i}$ represents distribution of each class in `a`.\n",
    "\n",
    "def IG(D, a):\n",
    "    '''\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "    '''\n",
    "    total = 0\n",
    "    for i in a:\n",
    "        total += sum(i) / sum(D) * entropy(i)\n",
    "    return entropy(D) - total\n",
    "\n",
    "\n",
    "# Test the function\n",
    "# Set of example of the dataset - distribution of classes\n",
    "test_dist = [6, 6] # Yes, No\n",
    "# Attribute, number of members (feature)\n",
    "test_attr = [ [4,0], [2,4], [0,2] ] # class1, class2, class3 of attr1 according to YES/NO classes in test_dist\n",
    "\n",
    "print(IG(test_dist, test_attr))\n",
    "\n",
    "# 0.5408520829727552"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
